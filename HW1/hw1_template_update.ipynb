{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Te27fi-0pP"
      },
      "source": [
        "# **HW1: Regression**\n",
        "In *assignment 1*, you need to finish:\n",
        "\n",
        "1.  Basic Part: Implement two regression models to predict the Systolic blood pressure (SBP) of a patient. You will need to implement **both Matrix Inversion and Gradient Descent**.\n",
        "\n",
        "\n",
        "> *   Step 1: Split Data\n",
        "> *   Step 2: Preprocess Data\n",
        "> *   Step 3: Implement Regression\n",
        "> *   Step 4: Make Prediction\n",
        "> *   Step 5: Train Model and Generate Result\n",
        "\n",
        "2.  Advanced Part: Implement one regression model to predict the SBP of multiple patients in a different way than the basic part. You can choose **either** of the two methods for this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDdnos-4uUv"
      },
      "source": [
        "# **1. Basic Part (55%)**\n",
        "In the first part, you need to implement the regression to predict SBP from the given DBP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_EVqWlB-DTF"
      },
      "source": [
        "## 1.1 Matrix Inversion Method (25%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_mi.csv**\n",
        "*   Print your coefficient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCR7vk9BFkf"
      },
      "source": [
        "### *Import Packages*\n",
        "\n",
        "> Note: You **cannot** import any other package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 746,
      "metadata": {
        "id": "HL5XjqFf4wSj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnWjrzi0dMPz"
      },
      "source": [
        "### *Global attributes*\n",
        "Define the global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 747,
      "metadata": {
        "id": "EWLDPOlHBbcK"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_basic_mi.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsFC-cvqIcYK"
      },
      "source": [
        "You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 748,
      "metadata": {
        "id": "OUbS2BEgcut6"
      },
      "outputs": [],
      "source": [
        "def MAPE(y_ans, y_prediction):\n",
        "    return np.mean(np.abs((y_ans - y_prediction) / y_ans)) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoRFoQjBW5S"
      },
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 749,
      "metadata": {
        "id": "dekR1KnqBtI6"
      },
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "    training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "    training_datalist = np.delete(training_datalist, 0, 0)\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "    testing_datalist = np.array(list(csv.reader(csvfile)))\n",
        "    testing_datalist = np.delete(testing_datalist, 0, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYPuikLCFx4"
      },
      "source": [
        "### *Implement the Regression Model*\n",
        "\n",
        "> Note: It is recommended to use the functions we defined, you can also define your own functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWwdx06JNEYs"
      },
      "source": [
        "#### Step 1: Split Data\n",
        "Split data in *training_datalist* into training dataset and validation dataset\n",
        "* Validation dataset is used to validate your own model without the testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 750,
      "metadata": {
        "id": "USDciENcB-5F"
      },
      "outputs": [],
      "source": [
        "def split_data(training_datalist: np.array) -> (np.array, np.array):\n",
        "    train_nums = int(len(training_datalist) * 0.8)\n",
        "    return training_datalist[ : train_nums], training_datalist[train_nums : ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-3Qln4aNgVy"
      },
      "source": [
        "#### Step 2: Preprocess Data\n",
        "Handle the unreasonable data\n",
        "> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 751,
      "metadata": {
        "id": "XXvW1n_5NkQ5"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(training_datalist: np.array, boundary: int) -> np.array:\n",
        "    train_nums = len(training_datalist)\n",
        "\n",
        "    dbp = np.array([[data[0]] for data in training_datalist]).astype(float)\n",
        "    sbp = np.array([[data[1]] for data in training_datalist]).astype(float)\n",
        "\n",
        "    dbp_mean = np.mean(dbp)\n",
        "    dbp_std = np.std(dbp)\n",
        "    sbp_mean = np.mean(sbp)\n",
        "    sbp_std = np.std(sbp)\n",
        "\n",
        "    i = -1\n",
        "    while i < train_nums - 1:\n",
        "        i += 1\n",
        "\n",
        "        dbp_below_threshold = dbp[i] < dbp_mean - boundary * dbp_std\n",
        "        dbp_above_threshold = dbp[i] > dbp_mean + boundary * dbp_std\n",
        "        sbp_below_threshold = sbp[i] < sbp_mean - boundary * sbp_std\n",
        "        sbp_above_threshold = sbp[i] > sbp_mean + boundary * sbp_std\n",
        "\n",
        "        if (dbp_below_threshold or dbp_above_threshold) or (sbp_below_threshold or sbp_above_threshold):\n",
        "            training_datalist = np.delete(training_datalist, i, 0)\n",
        "            train_nums -= 1\n",
        "            i -= 1\n",
        "\n",
        "    return training_datalist.astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDLpJmQUN3V6"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Matrix Inversion to finish this part\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 752,
      "metadata": {
        "id": "Tx9n1_23N8C0"
      },
      "outputs": [],
      "source": [
        "def matrix_inversion(train_set: np.array) -> np.array:\n",
        "    x = np.array([[1, data[0]] for data in train_set]).astype(float)\n",
        "    y_ans = np.array([[data[1]] for data in train_set]).astype(float)\n",
        "    return np.linalg.inv(x.T.dot(x)).dot(x.T).dot(y_ans)\n",
        "\n",
        "def validation(validate_set: np.array, coefficient: np.array) -> str:\n",
        "    validate_x = np.array([[1, data[0]] for data in validate_set])\n",
        "    validate_y = np.array([[data[1]] for data in validate_set])\n",
        "    validate_y_prediction = validate_x.dot(coefficient).astype(int)\n",
        "    return f'MAPE of validation set : {MAPE(validate_y, validate_y_prediction)}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NxRNFwyN8xd"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "Make prediction of testing dataset and store the value in *output_datalist*\n",
        "The final *output_datalist* should look something like this \n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 753,
      "metadata": {
        "id": "EKlDIC2-N_lk"
      },
      "outputs": [],
      "source": [
        "def make_prediction(testing_datalist: np.array, coefficient: np.array) -> np.array:\n",
        "    testing_x = np.array([[1, data[0]] for data in testing_datalist]).astype(float)\n",
        "    return testing_x.dot(coefficient).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCd0Z6izOCwq"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 754,
      "metadata": {
        "id": "iCL92EPKOFIn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAPE of validation set : 5.000690216635821\n",
            "\n",
            "---------- Coefficient ----------\n",
            "0.9210072989326239 52.277057839995386 "
          ]
        }
      ],
      "source": [
        "train, validate = split_data(preprocess_data(training_datalist, 2))\n",
        "coefficient = matrix_inversion(train)\n",
        "output_datalist = make_prediction(testing_datalist, coefficient)\n",
        "\n",
        "print(validation(validate, coefficient))\n",
        "print('\\n' + '-' * 10 + ' Coefficient ' + '-' * 10)\n",
        "\n",
        "for coe in coefficient[::-1]:\n",
        "    print(coe[0], end = ' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Jhd8wAOk3D"
      },
      "source": [
        "### *Write the Output File*\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 755,
      "metadata": {
        "id": "tYQVYLlKOtDB"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "\twriter = csv.writer(csvfile)\n",
        "\tfor row in output_datalist:\n",
        "\t\twriter.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3WOhglA9ML"
      },
      "source": [
        "## 1.2 Gradient Descent Method (30%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_gd.csv**\n",
        "*   Output your coefficient update in a csv file **hw1_basic_coefficient.csv**\n",
        "*   Print your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkMqa_xjXhEv"
      },
      "source": [
        "### *Global attributes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 756,
      "metadata": {
        "id": "wNZtRWUeXpEu"
      },
      "outputs": [],
      "source": [
        "output_dataroot = 'hw1_basic_gd.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "coefficient_output_dataroot = 'hw1_basic_coefficient.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']\n",
        "\n",
        "coefficient_output = [] # Your coefficient update during gradient descent\n",
        "                   # Should be a (number of iterations * number_of coefficient) matrix\n",
        "                   # The format of each row should be ['w0', 'w1', ...., 'wn']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5DeHxdLdai3"
      },
      "source": [
        "Your own global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 757,
      "metadata": {
        "id": "_2IO5tYSdaFd"
      },
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "    training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "    training_datalist = np.delete(training_datalist, 0, 0)\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "    testing_datalist = np.array(list(csv.reader(csvfile)))\n",
        "    testing_datalist = np.delete(testing_datalist, 0, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVBLT1aqXuW0"
      },
      "source": [
        "### *Implement the Regression Model*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPWpcOnXhCZ"
      },
      "source": [
        "#### Step 1: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 758,
      "metadata": {
        "id": "1PEf_qGvYHu0"
      },
      "outputs": [],
      "source": [
        "def split_data(training_datalist: np.array) -> (np.array, np.array):\n",
        "    train_nums = int(len(training_datalist) * 0.8)\n",
        "    return training_datalist[ : train_nums], training_datalist[train_nums : ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpSoPDPKX56w"
      },
      "source": [
        "#### Step 2: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 759,
      "metadata": {
        "id": "uLTXOWRwYHiS"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(training_datalist: np.array, width: float) -> np.array:\n",
        "    train_nums = len(training_datalist)\n",
        "\n",
        "    dbp = np.array([[data[0]] for data in training_datalist]).astype(float)\n",
        "    sbp = np.array([[data[1]] for data in training_datalist]).astype(float)\n",
        "\n",
        "    dbp_mean = np.mean(dbp)\n",
        "    dbp_std = np.std(dbp)\n",
        "    sbp_mean = np.mean(sbp)\n",
        "    sbp_std = np.std(sbp)\n",
        "\n",
        "    i = -1\n",
        "    while i < train_nums - 1:\n",
        "        i += 1\n",
        "\n",
        "        dbp_below_threshold = dbp[i] < dbp_mean - width * dbp_std\n",
        "        dbp_above_threshold = dbp[i] > dbp_mean + width * dbp_std\n",
        "        sbp_below_threshold = sbp[i] < sbp_mean - width * sbp_std\n",
        "        sbp_above_threshold = sbp[i] > sbp_mean + width * sbp_std\n",
        "\n",
        "        if (dbp_below_threshold or dbp_above_threshold) or (sbp_below_threshold or sbp_above_threshold):\n",
        "            training_datalist = np.delete(training_datalist, i, 0)\n",
        "            train_nums -= 1\n",
        "            i -= 1\n",
        "\n",
        "    return training_datalist.astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_y82gXX6a-"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Gradient Descent to finish this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 760,
      "metadata": {
        "id": "-635Ee00YHTE"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(epoch: int, learning_rate: float, train_set: np.array) -> np.array:\n",
        "    def gradient_of_loss(y_ans: np.array, y_predict: np.array, x: np.array) -> np.array: \n",
        "        return (-2 * (y_ans - y_predict)).T.dot(x).T\n",
        "    \n",
        "    coefficient = np.array([[50], [0.9]]).astype(float)\n",
        "    x = np.array([[1, data[0]] for data in train_set]).astype(float)\n",
        "    y_ans = np.array([[data[1]] for data in train_set]).astype(float)\n",
        "    y_predict = x.dot(coefficient).astype(float)\n",
        "\n",
        "    for e in range(epoch):\n",
        "        coefficient -= learning_rate * gradient_of_loss(y_ans, y_predict, x)\n",
        "        y_predict = x.dot(coefficient)\n",
        "        coefficient_output.append(coefficient)\n",
        "        print(f'Epoch {e + 1:2} : MAPE = {MAPE(y_ans, y_predict)}')\n",
        "    \n",
        "    return coefficient\n",
        "    \n",
        "def validation(validate_set: np.array, coefficient: np.array) -> str:\n",
        "    validate_x = np.array([[1, data[0]] for data in validate_set])\n",
        "    validate_y = np.array([[data[1]] for data in validate_set])\n",
        "    validate_y_prediction = validate_x.dot(coefficient).astype(int)\n",
        "    return f'MAPE of validation set : {MAPE(validate_y, validate_y_prediction)}'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLuPxs2ZX21S"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "\n",
        "Make prediction of testing dataset and store the values in *output_datalist*\n",
        "The final *output_datalist* should look something like this \n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP\n",
        "\n",
        "Remember to also store your coefficient update in *coefficient_output*\n",
        "The final *coefficient_output* should look something like this\n",
        "> [ [1, 0, 3, 5], ... , [0.1, 0.3, 0.2, 0.5] ] where each row contains the [w0, w1, ..., wn] of your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 761,
      "metadata": {
        "id": "8pnNDlQeYGtE"
      },
      "outputs": [],
      "source": [
        "def make_prediction(testing_datalist: np.array, coefficient: np.array) -> np.array:\n",
        "    testing_x = np.array([[1, data[0]] for data in testing_datalist]).astype(float)\n",
        "    return testing_x.dot(coefficient).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IScbxxMAYAgZ"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 762,
      "metadata": {
        "id": "90EisOc7YG-N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1 : MAPE = 4.8884711898988344\n",
            "Epoch  2 : MAPE = 4.92717997279329\n",
            "Epoch  3 : MAPE = 4.943203113903013\n",
            "Epoch  4 : MAPE = 4.9476709834159145\n",
            "Epoch  5 : MAPE = 4.948830525460115\n",
            "Epoch  6 : MAPE = 4.949120445821383\n",
            "Epoch  7 : MAPE = 4.94919295082846\n",
            "Epoch  8 : MAPE = 4.949211099513473\n",
            "Epoch  9 : MAPE = 4.949215658497023\n",
            "Epoch 10 : MAPE = 4.949216819899923\n",
            "Epoch 11 : MAPE = 4.949217131868798\n",
            "Epoch 12 : MAPE = 4.9492172314694125\n",
            "Epoch 13 : MAPE = 4.9492172779754835\n",
            "Epoch 14 : MAPE = 4.949217311207263\n",
            "Epoch 15 : MAPE = 4.949217341120262\n",
            "Epoch 16 : MAPE = 4.949217370203477\n",
            "Epoch 17 : MAPE = 4.949217399079183\n",
            "Epoch 18 : MAPE = 4.949217427902959\n",
            "Epoch 19 : MAPE = 4.949217456713698\n",
            "Epoch 20 : MAPE = 4.949217485521125\n",
            "Epoch 21 : MAPE = 4.949217514327674\n",
            "Epoch 22 : MAPE = 4.949217543133948\n",
            "Epoch 23 : MAPE = 4.9492175719401015\n",
            "Epoch 24 : MAPE = 4.949217600746173\n",
            "Epoch 25 : MAPE = 4.949217629552173\n",
            "Epoch 26 : MAPE = 4.949217658358101\n",
            "Epoch 27 : MAPE = 4.949217687163961\n",
            "Epoch 28 : MAPE = 4.949217715969748\n",
            "Epoch 29 : MAPE = 4.949217744775466\n",
            "Epoch 30 : MAPE = 4.9492177735811165\n",
            "Epoch 31 : MAPE = 4.949217802386697\n",
            "Epoch 32 : MAPE = 4.949217831192207\n",
            "Epoch 33 : MAPE = 4.9492178599976455\n",
            "Epoch 34 : MAPE = 4.949217888803017\n",
            "Epoch 35 : MAPE = 4.949217917608316\n",
            "Epoch 36 : MAPE = 4.949217946413546\n",
            "Epoch 37 : MAPE = 4.9492179752187075\n",
            "Epoch 38 : MAPE = 4.949218004023799\n",
            "Epoch 39 : MAPE = 4.949218032828819\n",
            "Epoch 40 : MAPE = 4.949218061633771\n",
            "Epoch 41 : MAPE = 4.949218090438652\n",
            "Epoch 42 : MAPE = 4.949218119243463\n",
            "Epoch 43 : MAPE = 4.949218148048207\n",
            "Epoch 44 : MAPE = 4.949218176852878\n",
            "Epoch 45 : MAPE = 4.949218205657482\n",
            "Epoch 46 : MAPE = 4.949218234462012\n",
            "Epoch 47 : MAPE = 4.949218263266475\n",
            "Epoch 48 : MAPE = 4.949218292070869\n",
            "Epoch 49 : MAPE = 4.949218320875193\n",
            "Epoch 50 : MAPE = 4.949218349679445\n",
            "Epoch 51 : MAPE = 4.9492183784836286\n",
            "Epoch 52 : MAPE = 4.949218407287743\n",
            "Epoch 53 : MAPE = 4.949218436091788\n",
            "Epoch 54 : MAPE = 4.949218464895762\n",
            "Epoch 55 : MAPE = 4.949218493699666\n",
            "Epoch 56 : MAPE = 4.949218522503502\n",
            "Epoch 57 : MAPE = 4.949218551307264\n",
            "Epoch 58 : MAPE = 4.949218580110961\n",
            "Epoch 59 : MAPE = 4.949218608914588\n",
            "Epoch 60 : MAPE = 4.949218637718145\n",
            "Epoch 61 : MAPE = 4.94921866652163\n",
            "Epoch 62 : MAPE = 4.949218695325044\n",
            "Epoch 63 : MAPE = 4.949218724128393\n",
            "Epoch 64 : MAPE = 4.949218752931668\n",
            "Epoch 65 : MAPE = 4.949218781734875\n",
            "Epoch 66 : MAPE = 4.949218810538011\n",
            "Epoch 67 : MAPE = 4.9492188393410785\n",
            "Epoch 68 : MAPE = 4.949218868144076\n",
            "Epoch 69 : MAPE = 4.949218896947006\n",
            "Epoch 70 : MAPE = 4.94921892574986\n",
            "Epoch 71 : MAPE = 4.9492189545526495\n",
            "Epoch 72 : MAPE = 4.949218983355367\n",
            "Epoch 73 : MAPE = 4.949219012158015\n",
            "Epoch 74 : MAPE = 4.949219040960596\n",
            "Epoch 75 : MAPE = 4.949219069763104\n",
            "Epoch 76 : MAPE = 4.949219098565543\n",
            "Epoch 77 : MAPE = 4.949219127367913\n",
            "Epoch 78 : MAPE = 4.949219156170212\n",
            "Epoch 79 : MAPE = 4.949219184972443\n",
            "Epoch 80 : MAPE = 4.9492192137746\n",
            "Epoch 81 : MAPE = 4.949219242576692\n",
            "Epoch 82 : MAPE = 4.949219271378713\n",
            "Epoch 83 : MAPE = 4.949219300180663\n",
            "Epoch 84 : MAPE = 4.949219328982545\n",
            "Epoch 85 : MAPE = 4.949219357784356\n",
            "Epoch 86 : MAPE = 4.949219386586097\n",
            "Epoch 87 : MAPE = 4.949219415387769\n",
            "Epoch 88 : MAPE = 4.949219444189372\n",
            "Epoch 89 : MAPE = 4.949219472990904\n",
            "Epoch 90 : MAPE = 4.949219501792364\n",
            "Epoch 91 : MAPE = 4.949219530593758\n",
            "Epoch 92 : MAPE = 4.949219559395079\n",
            "Epoch 93 : MAPE = 4.949219588196334\n",
            "Epoch 94 : MAPE = 4.949219616997515\n",
            "Epoch 95 : MAPE = 4.94921964579863\n",
            "Epoch 96 : MAPE = 4.949219674599672\n",
            "Epoch 97 : MAPE = 4.949219703400646\n",
            "Epoch 98 : MAPE = 4.949219732201552\n",
            "Epoch 99 : MAPE = 4.949219761002386\n",
            "Epoch 100 : MAPE = 4.94921978980315\n",
            "Epoch 101 : MAPE = 4.949219818603845\n",
            "Epoch 102 : MAPE = 4.94921984740447\n",
            "Epoch 103 : MAPE = 4.949219876205024\n",
            "Epoch 104 : MAPE = 4.949219905005511\n",
            "Epoch 105 : MAPE = 4.949219933805925\n",
            "Epoch 106 : MAPE = 4.949219962606271\n",
            "Epoch 107 : MAPE = 4.949219991406548\n",
            "Epoch 108 : MAPE = 4.949220020206755\n",
            "Epoch 109 : MAPE = 4.949220049006891\n",
            "Epoch 110 : MAPE = 4.949220077806958\n",
            "Epoch 111 : MAPE = 4.949220106606955\n",
            "Epoch 112 : MAPE = 4.949220135406881\n",
            "Epoch 113 : MAPE = 4.94922016420674\n",
            "Epoch 114 : MAPE = 4.949220193006527\n",
            "Epoch 115 : MAPE = 4.949220221806244\n",
            "Epoch 116 : MAPE = 4.949220250605895\n",
            "Epoch 117 : MAPE = 4.949220279405471\n",
            "Epoch 118 : MAPE = 4.949220308204978\n",
            "Epoch 119 : MAPE = 4.9492203370044185\n",
            "Epoch 120 : MAPE = 4.949220365803787\n",
            "Epoch 121 : MAPE = 4.949220394603087\n",
            "Epoch 122 : MAPE = 4.949220423402316\n",
            "Epoch 123 : MAPE = 4.949220452201477\n",
            "Epoch 124 : MAPE = 4.949220481000566\n",
            "Epoch 125 : MAPE = 4.949220509799587\n",
            "Epoch 126 : MAPE = 4.949220538598538\n",
            "Epoch 127 : MAPE = 4.949220567397416\n",
            "Epoch 128 : MAPE = 4.949220596196229\n",
            "Epoch 129 : MAPE = 4.949220624994971\n",
            "Epoch 130 : MAPE = 4.949220653793642\n",
            "Epoch 131 : MAPE = 4.949220682592244\n",
            "Epoch 132 : MAPE = 4.949220711390774\n",
            "Epoch 133 : MAPE = 4.9492207401892365\n",
            "Epoch 134 : MAPE = 4.9492207689876295\n",
            "Epoch 135 : MAPE = 4.9492207977859515\n",
            "Epoch 136 : MAPE = 4.949220826584204\n",
            "Epoch 137 : MAPE = 4.9492208553823875\n",
            "Epoch 138 : MAPE = 4.9492208841805\n",
            "Epoch 139 : MAPE = 4.949220912978544\n",
            "Epoch 140 : MAPE = 4.949220941776518\n",
            "Epoch 141 : MAPE = 4.949220970574421\n",
            "Epoch 142 : MAPE = 4.949220999372255\n",
            "Epoch 143 : MAPE = 4.949221028170021\n",
            "Epoch 144 : MAPE = 4.949221056967714\n",
            "Epoch 145 : MAPE = 4.9492210857653385\n",
            "Epoch 146 : MAPE = 4.949221114562895\n",
            "Epoch 147 : MAPE = 4.949221143360381\n",
            "Epoch 148 : MAPE = 4.949221172157795\n",
            "Epoch 149 : MAPE = 4.949221200955142\n",
            "Epoch 150 : MAPE = 4.949221229752416\n",
            "Epoch 151 : MAPE = 4.949221258549622\n",
            "Epoch 152 : MAPE = 4.949221287346759\n",
            "Epoch 153 : MAPE = 4.949221316143825\n",
            "Epoch 154 : MAPE = 4.949221344940823\n",
            "Epoch 155 : MAPE = 4.9492213737377515\n",
            "Epoch 156 : MAPE = 4.949221402534608\n",
            "Epoch 157 : MAPE = 4.949221431331397\n",
            "Epoch 158 : MAPE = 4.949221460128113\n",
            "Epoch 159 : MAPE = 4.949221488924761\n",
            "Epoch 160 : MAPE = 4.949221517721339\n",
            "Epoch 161 : MAPE = 4.9492215465178475\n",
            "Epoch 162 : MAPE = 4.949221575314288\n",
            "Epoch 163 : MAPE = 4.949221604110659\n",
            "Epoch 164 : MAPE = 4.949221632906955\n",
            "Epoch 165 : MAPE = 4.949221661703184\n",
            "Epoch 166 : MAPE = 4.949221690499344\n",
            "Epoch 167 : MAPE = 4.949221719295434\n",
            "Epoch 168 : MAPE = 4.949221748091456\n",
            "Epoch 169 : MAPE = 4.949221776887405\n",
            "Epoch 170 : MAPE = 4.949221805683286\n",
            "Epoch 171 : MAPE = 4.949221834479096\n",
            "Epoch 172 : MAPE = 4.94922186327484\n",
            "Epoch 173 : MAPE = 4.949221892070509\n",
            "Epoch 174 : MAPE = 4.949221920866111\n",
            "Epoch 175 : MAPE = 4.949221949661643\n",
            "Epoch 176 : MAPE = 4.949221978457105\n",
            "Epoch 177 : MAPE = 4.949222007252498\n",
            "Epoch 178 : MAPE = 4.94922203604782\n",
            "Epoch 179 : MAPE = 4.949222064843073\n",
            "Epoch 180 : MAPE = 4.949222093638255\n",
            "Epoch 181 : MAPE = 4.94922212243337\n",
            "Epoch 182 : MAPE = 4.949222151228412\n",
            "Epoch 183 : MAPE = 4.949222180023387\n",
            "Epoch 184 : MAPE = 4.94922220881829\n",
            "Epoch 185 : MAPE = 4.949222237613125\n",
            "Epoch 186 : MAPE = 4.949222266407889\n",
            "Epoch 187 : MAPE = 4.949222295202584\n",
            "Epoch 188 : MAPE = 4.949222323997208\n",
            "Epoch 189 : MAPE = 4.9492223527917645\n",
            "Epoch 190 : MAPE = 4.949222381586249\n",
            "Epoch 191 : MAPE = 4.949222410380667\n",
            "Epoch 192 : MAPE = 4.9492224391750135\n",
            "Epoch 193 : MAPE = 4.949222467969289\n",
            "Epoch 194 : MAPE = 4.949222496763495\n",
            "Epoch 195 : MAPE = 4.94922252555763\n",
            "Epoch 196 : MAPE = 4.949222554351697\n",
            "Epoch 197 : MAPE = 4.949222583145694\n",
            "Epoch 198 : MAPE = 4.949222611939621\n",
            "Epoch 199 : MAPE = 4.94922264073348\n",
            "Epoch 200 : MAPE = 4.949222669527267\n",
            "\n",
            "---------- MAPE ----------\n",
            "MAPE of validation set : 4.9843450987673075\n",
            "\n",
            "---------- Coefficient ----------\n",
            "0.9486378390461381 50.00169355057738 "
          ]
        }
      ],
      "source": [
        "train, validate = split_data(preprocess_data(training_datalist, 2))\n",
        "coefficient = gradient_descent(200, 0.0000005, train)\n",
        "output_datalist = make_prediction(testing_datalist, coefficient)\n",
        "\n",
        "print('\\n' + '-' * 10 + ' MAPE ' + '-' * 10)\n",
        "print(validation(validate, coefficient))\n",
        "print('\\n' + '-' * 10 + ' Coefficient ' + '-' * 10)\n",
        "\n",
        "for coe in coefficient[::-1]:\n",
        "    print(coe[0], end = ' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1DpV_HcYFpl"
      },
      "source": [
        "### *Write the Output File*\n",
        "\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "**Write the coefficient update to csv**\n",
        "> Format: 'w0', 'w1', ..., 'wn'\n",
        ">*   The number of columns is based on your number of coefficient\n",
        ">*   The number of row is based on your number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 763,
      "metadata": {
        "id": "NLSHgpDvDXNI"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "\twriter = csv.writer(csvfile)\n",
        "\tfor row in output_datalist:\n",
        "\t\twriter.writerow(row)\n",
        "\n",
        "with open(coefficient_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "\twriter = csv.writer(csvfile)\n",
        "\tfor row in coefficient_output:\n",
        "\t\twriter.writerow(row.flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4408qg4xMQ"
      },
      "source": [
        "# **2. Advanced Part (40%)**\n",
        "In the second part, you need to implement the regression in a different way than the basic part to help your predictions of multiple patients SBP.\n",
        "\n",
        "You can choose **either** Matrix Inversion or Gradient Descent method.\n",
        "\n",
        "The training data will be in **hw1_advanced_training.csv** and the testing data will be in **hw1_advanced_testing.csv**.\n",
        "\n",
        "Output your prediction in **hw1_advanced.csv**\n",
        "\n",
        "Notice:\n",
        "> You cannot import any other package other than those given\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Input the training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 764,
      "metadata": {
        "id": "v66HUClZcxaE"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_advanced_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_advanced_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_advanced.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 220 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. read the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 765,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_data() -> (list, list, np.array):\n",
        "    train_id = []\n",
        "    train_time = []\n",
        "    train_data = []\n",
        "\n",
        "    training_data = []\n",
        "    with open(training_dataroot, newline='') as csvfile:\n",
        "        training_data = list(csv.reader(csvfile))[1 : ]\n",
        "\n",
        "    train_id = [int(data[0]) for data in training_data]\n",
        "\n",
        "    train_data = np.array([data[2 : ] for data in training_data])\n",
        "    train_data[train_data == ''] = np.nan\n",
        "    train_data = train_data.astype(float)\n",
        "\n",
        "    for data in training_data:\n",
        "        time_list = data[1].split(' ')\n",
        "        days_ago = int(time_list[0])\n",
        "\n",
        "        time = time_list[2].split(':')\n",
        "        hour, minute = int(time[0]), int(time[1])\n",
        "        train_time.append([days_ago, hour, minute])\n",
        "\n",
        "    return train_id, train_time, train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. preprocess the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 766,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_incomplete_data(train_id: list, train_time: list, train_data: np.array) -> (list, list, np.array):\n",
        "    i = 0; train_nums = len(train_data)\n",
        "    while i < train_nums:\n",
        "        if np.sum(np.isnan(train_data[i])) >= 2:\n",
        "            train_data = np.delete(train_data, i, 0)\n",
        "            train_id.pop(i)\n",
        "            train_time.pop(i)\n",
        "            train_nums -= 1\n",
        "            i -= 1\n",
        "\n",
        "        if np.isnan(train_data[i][0]):\n",
        "            train_data[i][0] = train_data[i - 1][0]\n",
        "        elif np.isnan(train_data[i][1]):\n",
        "            train_data[i][1] = train_data[i - 1][1]\n",
        "        elif np.isnan(train_data[i][2]):\n",
        "            train_data[i][2] = train_data[i - 1][2]\n",
        "        elif np.isnan(train_data[i][3]):\n",
        "            train_data[i][3] = train_data[i - 1][3]\n",
        "\n",
        "        i += 1\n",
        "    return train_id, train_time, train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 767,
      "metadata": {},
      "outputs": [],
      "source": [
        "def divide_data(train_id: list, train_time: list, train_data: np.array) -> list:\n",
        "    ## We first divide the data into group by id\n",
        "    last_id = train_id[0]; group = []; tem_group = []\n",
        "    for i, data in enumerate(train_id):\n",
        "        tem_group.append(train_data[i])\n",
        "        if data != last_id:\n",
        "            group.append(tem_group)\n",
        "            tem_group = []\n",
        "            last_id = data\n",
        "    group.append(tem_group) # list-list-ndarray\n",
        "\n",
        "    ## Then we divide the data into group by time\n",
        "    new_group = []\n",
        "    for people in group:\n",
        "        morning, noon, afternoon, evening, night = [], [], [], [], []\n",
        "\n",
        "        for i in range(len(people)):\n",
        "            clock = train_time.pop(0)[1]\n",
        "            \n",
        "            if clock >= 6 and clock < 10:\n",
        "                morning.append([clock, *train_data[i]])\n",
        "            elif clock >= 10 and clock < 14:\n",
        "                noon.append([clock, *train_data[i]])\n",
        "            elif clock >= 14 and clock < 18:\n",
        "                afternoon.append([clock, *train_data[i]])\n",
        "            elif clock >= 18 and clock < 22:\n",
        "                evening.append([clock, *train_data[i]])\n",
        "            else:\n",
        "                night.append([clock, *train_data[i]])\n",
        "        \n",
        "        new_group.append([morning, noon, afternoon, evening, night])\n",
        "    return new_group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 768,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_outliers(group: list, width: float) -> list:\n",
        "    new_group = []\n",
        "\n",
        "    for people in group:\n",
        "        new_people = []\n",
        "\n",
        "        for time in people:\n",
        "            new_time = []\n",
        "\n",
        "            temperature = np.array([data[1] for data in time])\n",
        "            temperature_mean = np.mean(temperature); temperature_std = np.std(temperature)\n",
        "\n",
        "            heartrate = np.array([data[2] for data in time])\n",
        "            heartrate_mean = np.mean(heartrate); heartrate_std = np.std(heartrate)\n",
        "\n",
        "            resprate = np.array([data[3] for data in time])\n",
        "            resprate_mean = np.mean(resprate); resprate_std = np.std(resprate)\n",
        "\n",
        "            o2sat = np.array([data[4] for data in time])\n",
        "            o2sat_mean = np.mean(o2sat); o2sat_std = np.std(o2sat)\n",
        "\n",
        "            sbp = np.array([data[5] for data in time])\n",
        "            sbp_mean = np.mean(sbp); sbp_std = np.std(sbp)\n",
        "\n",
        "            temp_beyond_threshold = np.abs(temperature - temperature_mean) > width * temperature_std\n",
        "            heartrate_beyond_threshold = np.abs(heartrate - heartrate_mean) > width * heartrate_std\n",
        "            resprate_beyond_threshold = np.abs(resprate - resprate_mean) > width * resprate_std\n",
        "            o2sat_beyond_threshold = np.abs(o2sat - o2sat_mean) > width * o2sat_std\n",
        "            sbp_beyond_threshold = np.abs(sbp - sbp_mean) > width * sbp_std\n",
        "\n",
        "            for i in range(len(time)):\n",
        "                if temp_beyond_threshold[i] or heartrate_beyond_threshold[i] or resprate_beyond_threshold[i] or o2sat_beyond_threshold[i] or sbp_beyond_threshold[i]:\n",
        "                    continue\n",
        "                new_time.append(time[i])\n",
        "            new_people.append(time)\n",
        "        new_group.append(new_people)\n",
        "\n",
        "    return new_group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 769,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(train_id: np.array, train_time: np.array, train_data: np.array, width: float) -> list:\n",
        "    '''\n",
        "    Assume the return value called list, then:\n",
        "        list: Whole data after preprocessing \\n\n",
        "        list[i]: Whole Data of the i-th person \\n\n",
        "        list[i][j]: Whole Data of the i-th person in the j-th time period \\n\n",
        "        list[i][j][k]: k-th Data of the i-th person in the j-th time period \\n\n",
        "        list[i][j][k][0]: Temperature of the k-th data of the i-th person in the j-th time period\n",
        "    '''\n",
        "    # remove the data with more than 2 elements missing\n",
        "    train_id, train_time, train_data = remove_incomplete_data(train_id, train_time, train_data)\n",
        "\n",
        "    # remove outliers\n",
        "    ## We first divide the data into group by id and time\n",
        "    group = divide_data(train_id, train_time, train_data)\n",
        "\n",
        "    ## Then we remove the data in each group once it has any outlier and return\n",
        "    return remove_outliers(group, width)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 770,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_descent(y_ans: np.array, y_predict: np.array, x: np.array) -> np.array:\n",
        "    return (-2 * (y_ans - y_predict)).T.dot(x).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Adam() -> np.array:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 771,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(train_set: np.array, epoch: int, learning_rate: float) -> np.array:\n",
        "    eta = learning_rate\n",
        "    batch_nums = len(train_set)\n",
        "    coefficient = np.array([[0], [0.5], [0], [1], [1], [0]]).astype(float) # const, clock, temperature, heartrate, resprate, o2sat\n",
        "    \n",
        "    for e in range(epoch):\n",
        "        ith_validate = 0\n",
        "\n",
        "        validate_set = []\n",
        "        for i ,batch in enumerate(train_set):\n",
        "            if i % batch_nums == ith_validate:\n",
        "                validate_set = batch\n",
        "                continue\n",
        "\n",
        "            x = np.array([[1, *data[ : -1]] for data in batch]).astype(float)\n",
        "            y_ans = np.array([[data[-1]] for data in batch]).astype(float)\n",
        "            y_predict = x.dot(coefficient).astype(float)\n",
        "\n",
        "            gradient = gradient_descent(y_ans, y_predict, x)\n",
        "            coefficient = Adam()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output your Prediction\n",
        "\n",
        "> your filename should be **hw1_advanced.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 772,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "  writer = csv.writer(csvfile)\n",
        "  for row in output_datalist:\n",
        "    writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtgCJU7FPeJL"
      },
      "source": [
        "# Report *(5%)*\n",
        "\n",
        "Report should be submitted as a pdf file **hw1_report.pdf**\n",
        "\n",
        "*   Briefly describe the difficulty you encountered\n",
        "*   Summarize your work and your reflections\n",
        "*   No more than one page\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlEE53_MPf4W"
      },
      "source": [
        "# Save the Code File\n",
        "Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Te27fi-0pP"
      },
      "source": [
        "# **HW1: Regression**\n",
        "In *assignment 1*, you need to finish:\n",
        "\n",
        "1.  Basic Part: Implement two regression models to predict the Systolic blood pressure (SBP) of a patient. You will need to implement **both Matrix Inversion and Gradient Descent**.\n",
        "\n",
        "\n",
        "> *   Step 1: Split Data\n",
        "> *   Step 2: Preprocess Data\n",
        "> *   Step 3: Implement Regression\n",
        "> *   Step 4: Make Prediction\n",
        "> *   Step 5: Train Model and Generate Result\n",
        "\n",
        "2.  Advanced Part: Implement one regression model to predict the SBP of multiple patients in a different way than the basic part. You can choose **either** of the two methods for this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDdnos-4uUv"
      },
      "source": [
        "# **1. Basic Part (55%)**\n",
        "In the first part, you need to implement the regression to predict SBP from the given DBP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_EVqWlB-DTF"
      },
      "source": [
        "## 1.1 Matrix Inversion Method (25%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_mi.csv**\n",
        "*   Print your coefficient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCR7vk9BFkf"
      },
      "source": [
        "### *Import Packages*\n",
        "\n",
        "> Note: You **cannot** import any other package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {
        "id": "HL5XjqFf4wSj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnWjrzi0dMPz"
      },
      "source": [
        "### *Global attributes*\n",
        "Define the global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {
        "id": "EWLDPOlHBbcK"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_basic_mi.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsFC-cvqIcYK"
      },
      "source": [
        "You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "metadata": {
        "id": "OUbS2BEgcut6"
      },
      "outputs": [],
      "source": [
        "def MAPE(y_ans, y_prediction):\n",
        "    return np.mean(np.abs((y_ans - y_prediction) / y_ans)) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoRFoQjBW5S"
      },
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {
        "id": "dekR1KnqBtI6"
      },
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "    training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "    training_datalist = np.delete(training_datalist, 0, 0)\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "    testing_datalist = np.array(list(csv.reader(csvfile)))\n",
        "    testing_datalist = np.delete(testing_datalist, 0, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYPuikLCFx4"
      },
      "source": [
        "### *Implement the Regression Model*\n",
        "\n",
        "> Note: It is recommended to use the functions we defined, you can also define your own functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWwdx06JNEYs"
      },
      "source": [
        "#### Step 1: Split Data\n",
        "Split data in *training_datalist* into training dataset and validation dataset\n",
        "* Validation dataset is used to validate your own model without the testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {
        "id": "USDciENcB-5F"
      },
      "outputs": [],
      "source": [
        "def split_data(training_datalist: np.array) -> (np.array, np.array):\n",
        "    train_nums = int(len(training_datalist) * 0.8)\n",
        "    return training_datalist[ : train_nums], training_datalist[train_nums : ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-3Qln4aNgVy"
      },
      "source": [
        "#### Step 2: Preprocess Data\n",
        "Handle the unreasonable data\n",
        "> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {
        "id": "XXvW1n_5NkQ5"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(training_datalist: np.array, boundary: int) -> np.array:\n",
        "    train_nums = len(training_datalist)\n",
        "\n",
        "    dbp = np.array([[data[0]] for data in training_datalist]).astype(float)\n",
        "    sbp = np.array([[data[1]] for data in training_datalist]).astype(float)\n",
        "\n",
        "    dbp_mean = np.mean(dbp)\n",
        "    dbp_std = np.std(dbp)\n",
        "    sbp_mean = np.mean(sbp)\n",
        "    sbp_std = np.std(sbp)\n",
        "\n",
        "    i = -1\n",
        "    while i < train_nums - 1:\n",
        "        i += 1\n",
        "\n",
        "        dbp_below_threshold = dbp[i] < dbp_mean - boundary * dbp_std\n",
        "        dbp_above_threshold = dbp[i] > dbp_mean + boundary * dbp_std\n",
        "        sbp_below_threshold = sbp[i] < sbp_mean - boundary * sbp_std\n",
        "        sbp_above_threshold = sbp[i] > sbp_mean + boundary * sbp_std\n",
        "\n",
        "        if (dbp_below_threshold or dbp_above_threshold) or (sbp_below_threshold or sbp_above_threshold):\n",
        "            training_datalist = np.delete(training_datalist, i, 0)\n",
        "            train_nums -= 1\n",
        "            i -= 1\n",
        "\n",
        "    return training_datalist.astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDLpJmQUN3V6"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Matrix Inversion to finish this part\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "id": "Tx9n1_23N8C0"
      },
      "outputs": [],
      "source": [
        "def matrix_inversion(train_set: np.array) -> np.array:\n",
        "    x = np.array([[1, data[0]] for data in train_set]).astype(float)\n",
        "    y_ans = np.array([[data[1]] for data in train_set]).astype(float)\n",
        "    return np.linalg.inv(x.T.dot(x)).dot(x.T).dot(y_ans)\n",
        "\n",
        "def validation(validate_set: np.array, coefficient: np.array) -> str:\n",
        "    validate_x = np.array([[1, data[0]] for data in validate_set])\n",
        "    validate_y = np.array([[data[1]] for data in validate_set])\n",
        "    validate_y_prediction = validate_x.dot(coefficient).astype(int)\n",
        "    return f'MAPE of validation set : {MAPE(validate_y, validate_y_prediction)}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NxRNFwyN8xd"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "Make prediction of testing dataset and store the value in *output_datalist*\n",
        "The final *output_datalist* should look something like this \n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {
        "id": "EKlDIC2-N_lk"
      },
      "outputs": [],
      "source": [
        "def make_prediction(testing_datalist: np.array, coefficient: np.array) -> np.array:\n",
        "    testing_x = np.array([[1, data[0]] for data in testing_datalist]).astype(float)\n",
        "    return testing_x.dot(coefficient).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCd0Z6izOCwq"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {
        "id": "iCL92EPKOFIn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MAPE of validation set : 5.000690216635821\n",
            "\n",
            "---------- Coefficient ----------\n",
            "0.9210072989326239 52.277057839995386 "
          ]
        }
      ],
      "source": [
        "train, validate = split_data(preprocess_data(training_datalist, 2))\n",
        "coefficient = matrix_inversion(train)\n",
        "output_datalist = make_prediction(testing_datalist, coefficient)\n",
        "\n",
        "print(validation(validate, coefficient))\n",
        "print('\\n' + '-' * 10 + ' Coefficient ' + '-' * 10)\n",
        "\n",
        "for coe in coefficient[::-1]:\n",
        "    print(coe[0], end = ' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Jhd8wAOk3D"
      },
      "source": [
        "### *Write the Output File*\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {
        "id": "tYQVYLlKOtDB"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "\twriter = csv.writer(csvfile)\n",
        "\tfor row in output_datalist:\n",
        "\t\twriter.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3WOhglA9ML"
      },
      "source": [
        "## 1.2 Gradient Descent Method (30%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_gd.csv**\n",
        "*   Output your coefficient update in a csv file **hw1_basic_coefficient.csv**\n",
        "*   Print your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkMqa_xjXhEv"
      },
      "source": [
        "### *Global attributes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {
        "id": "wNZtRWUeXpEu"
      },
      "outputs": [],
      "source": [
        "output_dataroot = 'hw1_basic_gd.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "coefficient_output_dataroot = 'hw1_basic_coefficient.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']\n",
        "\n",
        "coefficient_output = [] # Your coefficient update during gradient descent\n",
        "                   # Should be a (number of iterations * number_of coefficient) matrix\n",
        "                   # The format of each row should be ['w0', 'w1', ...., 'wn']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5DeHxdLdai3"
      },
      "source": [
        "Your own global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {
        "id": "_2IO5tYSdaFd"
      },
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "    training_datalist = np.array(list(csv.reader(csvfile)))\n",
        "    training_datalist = np.delete(training_datalist, 0, 0)\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "    testing_datalist = np.array(list(csv.reader(csvfile)))\n",
        "    testing_datalist = np.delete(testing_datalist, 0, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVBLT1aqXuW0"
      },
      "source": [
        "### *Implement the Regression Model*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPWpcOnXhCZ"
      },
      "source": [
        "#### Step 1: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {
        "id": "1PEf_qGvYHu0"
      },
      "outputs": [],
      "source": [
        "def split_data(training_datalist: np.array) -> (np.array, np.array):\n",
        "    train_nums = int(len(training_datalist) * 0.8)\n",
        "    return training_datalist[ : train_nums], training_datalist[train_nums : ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpSoPDPKX56w"
      },
      "source": [
        "#### Step 2: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 388,
      "metadata": {
        "id": "uLTXOWRwYHiS"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(training_datalist: np.array, width: float) -> np.array:\n",
        "    train_nums = len(training_datalist)\n",
        "\n",
        "    dbp = np.array([[data[0]] for data in training_datalist]).astype(float)\n",
        "    sbp = np.array([[data[1]] for data in training_datalist]).astype(float)\n",
        "\n",
        "    dbp_mean = np.mean(dbp)\n",
        "    dbp_std = np.std(dbp)\n",
        "    sbp_mean = np.mean(sbp)\n",
        "    sbp_std = np.std(sbp)\n",
        "\n",
        "    i = -1\n",
        "    while i < train_nums - 1:\n",
        "        i += 1\n",
        "\n",
        "        dbp_below_threshold = dbp[i] < dbp_mean - width * dbp_std\n",
        "        dbp_above_threshold = dbp[i] > dbp_mean + width * dbp_std\n",
        "        sbp_below_threshold = sbp[i] < sbp_mean - width * sbp_std\n",
        "        sbp_above_threshold = sbp[i] > sbp_mean + width * sbp_std\n",
        "\n",
        "        if (dbp_below_threshold or dbp_above_threshold) or (sbp_below_threshold or sbp_above_threshold):\n",
        "            training_datalist = np.delete(training_datalist, i, 0)\n",
        "            train_nums -= 1\n",
        "            i -= 1\n",
        "\n",
        "    return training_datalist.astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_y82gXX6a-"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Gradient Descent to finish this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 389,
      "metadata": {
        "id": "-635Ee00YHTE"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(epoch: int, learning_rate: float, train_set: np.array) -> np.array:\n",
        "    def gradient_of_loss(y_ans: np.array, y_predict: np.array, x: np.array) -> np.array: \n",
        "        return (-2 * (y_ans - y_predict)).T.dot(x).T\n",
        "    \n",
        "    coefficient = np.array([[50], [0.9]]).astype(float)\n",
        "    x = np.array([[1, data[0]] for data in train_set]).astype(float)\n",
        "    y_ans = np.array([[data[1]] for data in train_set]).astype(float)\n",
        "    y_predict = x.dot(coefficient).astype(float)\n",
        "\n",
        "    for e in range(epoch):\n",
        "        coefficient -= learning_rate * gradient_of_loss(y_ans, y_predict, x)\n",
        "        y_predict = x.dot(coefficient)\n",
        "        coefficient_output.append(coefficient[::-1])\n",
        "        print(f'Epoch {e + 1:2} : MAPE = {MAPE(y_ans, y_predict)}')\n",
        "    \n",
        "    return coefficient\n",
        "    \n",
        "def validation(validate_set: np.array, coefficient: np.array) -> str:\n",
        "    validate_x = np.array([[1, data[0]] for data in validate_set])\n",
        "    validate_y = np.array([[data[1]] for data in validate_set])\n",
        "    validate_y_prediction = validate_x.dot(coefficient).astype(int)\n",
        "    return f'MAPE of validation set : {MAPE(validate_y, validate_y_prediction)}'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLuPxs2ZX21S"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "\n",
        "Make prediction of testing dataset and store the values in *output_datalist*\n",
        "The final *output_datalist* should look something like this \n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP\n",
        "\n",
        "Remember to also store your coefficient update in *coefficient_output*\n",
        "The final *coefficient_output* should look something like this\n",
        "> [ [1, 0, 3, 5], ... , [0.1, 0.3, 0.2, 0.5] ] where each row contains the [w0, w1, ..., wn] of your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 390,
      "metadata": {
        "id": "8pnNDlQeYGtE"
      },
      "outputs": [],
      "source": [
        "def make_prediction(testing_datalist: np.array, coefficient: np.array) -> np.array:\n",
        "    testing_x = np.array([[1, data[0]] for data in testing_datalist]).astype(float)\n",
        "    return testing_x.dot(coefficient).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IScbxxMAYAgZ"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {
        "id": "90EisOc7YG-N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1 : MAPE = 4.8884711898988344\n",
            "Epoch  2 : MAPE = 4.92717997279329\n",
            "Epoch  3 : MAPE = 4.943203113903013\n",
            "Epoch  4 : MAPE = 4.9476709834159145\n",
            "Epoch  5 : MAPE = 4.948830525460115\n",
            "Epoch  6 : MAPE = 4.949120445821383\n",
            "Epoch  7 : MAPE = 4.94919295082846\n",
            "Epoch  8 : MAPE = 4.949211099513473\n",
            "Epoch  9 : MAPE = 4.949215658497023\n",
            "Epoch 10 : MAPE = 4.949216819899923\n",
            "Epoch 11 : MAPE = 4.949217131868798\n",
            "Epoch 12 : MAPE = 4.9492172314694125\n",
            "Epoch 13 : MAPE = 4.9492172779754835\n",
            "Epoch 14 : MAPE = 4.949217311207263\n",
            "Epoch 15 : MAPE = 4.949217341120262\n",
            "Epoch 16 : MAPE = 4.949217370203477\n",
            "Epoch 17 : MAPE = 4.949217399079183\n",
            "Epoch 18 : MAPE = 4.949217427902959\n",
            "Epoch 19 : MAPE = 4.949217456713698\n",
            "Epoch 20 : MAPE = 4.949217485521125\n",
            "Epoch 21 : MAPE = 4.949217514327674\n",
            "Epoch 22 : MAPE = 4.949217543133948\n",
            "Epoch 23 : MAPE = 4.9492175719401015\n",
            "Epoch 24 : MAPE = 4.949217600746173\n",
            "Epoch 25 : MAPE = 4.949217629552173\n",
            "Epoch 26 : MAPE = 4.949217658358101\n",
            "Epoch 27 : MAPE = 4.949217687163961\n",
            "Epoch 28 : MAPE = 4.949217715969748\n",
            "Epoch 29 : MAPE = 4.949217744775466\n",
            "Epoch 30 : MAPE = 4.9492177735811165\n",
            "Epoch 31 : MAPE = 4.949217802386697\n",
            "Epoch 32 : MAPE = 4.949217831192207\n",
            "Epoch 33 : MAPE = 4.9492178599976455\n",
            "Epoch 34 : MAPE = 4.949217888803017\n",
            "Epoch 35 : MAPE = 4.949217917608316\n",
            "Epoch 36 : MAPE = 4.949217946413546\n",
            "Epoch 37 : MAPE = 4.9492179752187075\n",
            "Epoch 38 : MAPE = 4.949218004023799\n",
            "Epoch 39 : MAPE = 4.949218032828819\n",
            "Epoch 40 : MAPE = 4.949218061633771\n",
            "Epoch 41 : MAPE = 4.949218090438652\n",
            "Epoch 42 : MAPE = 4.949218119243463\n",
            "Epoch 43 : MAPE = 4.949218148048207\n",
            "Epoch 44 : MAPE = 4.949218176852878\n",
            "Epoch 45 : MAPE = 4.949218205657482\n",
            "Epoch 46 : MAPE = 4.949218234462012\n",
            "Epoch 47 : MAPE = 4.949218263266475\n",
            "Epoch 48 : MAPE = 4.949218292070869\n",
            "Epoch 49 : MAPE = 4.949218320875193\n",
            "Epoch 50 : MAPE = 4.949218349679445\n",
            "Epoch 51 : MAPE = 4.9492183784836286\n",
            "Epoch 52 : MAPE = 4.949218407287743\n",
            "Epoch 53 : MAPE = 4.949218436091788\n",
            "Epoch 54 : MAPE = 4.949218464895762\n",
            "Epoch 55 : MAPE = 4.949218493699666\n",
            "Epoch 56 : MAPE = 4.949218522503502\n",
            "Epoch 57 : MAPE = 4.949218551307264\n",
            "Epoch 58 : MAPE = 4.949218580110961\n",
            "Epoch 59 : MAPE = 4.949218608914588\n",
            "Epoch 60 : MAPE = 4.949218637718145\n",
            "Epoch 61 : MAPE = 4.94921866652163\n",
            "Epoch 62 : MAPE = 4.949218695325044\n",
            "Epoch 63 : MAPE = 4.949218724128393\n",
            "Epoch 64 : MAPE = 4.949218752931668\n",
            "Epoch 65 : MAPE = 4.949218781734875\n",
            "Epoch 66 : MAPE = 4.949218810538011\n",
            "Epoch 67 : MAPE = 4.9492188393410785\n",
            "Epoch 68 : MAPE = 4.949218868144076\n",
            "Epoch 69 : MAPE = 4.949218896947006\n",
            "Epoch 70 : MAPE = 4.94921892574986\n",
            "Epoch 71 : MAPE = 4.9492189545526495\n",
            "Epoch 72 : MAPE = 4.949218983355367\n",
            "Epoch 73 : MAPE = 4.949219012158015\n",
            "Epoch 74 : MAPE = 4.949219040960596\n",
            "Epoch 75 : MAPE = 4.949219069763104\n",
            "Epoch 76 : MAPE = 4.949219098565543\n",
            "Epoch 77 : MAPE = 4.949219127367913\n",
            "Epoch 78 : MAPE = 4.949219156170212\n",
            "Epoch 79 : MAPE = 4.949219184972443\n",
            "Epoch 80 : MAPE = 4.9492192137746\n",
            "Epoch 81 : MAPE = 4.949219242576692\n",
            "Epoch 82 : MAPE = 4.949219271378713\n",
            "Epoch 83 : MAPE = 4.949219300180663\n",
            "Epoch 84 : MAPE = 4.949219328982545\n",
            "Epoch 85 : MAPE = 4.949219357784356\n",
            "Epoch 86 : MAPE = 4.949219386586097\n",
            "Epoch 87 : MAPE = 4.949219415387769\n",
            "Epoch 88 : MAPE = 4.949219444189372\n",
            "Epoch 89 : MAPE = 4.949219472990904\n",
            "Epoch 90 : MAPE = 4.949219501792364\n",
            "Epoch 91 : MAPE = 4.949219530593758\n",
            "Epoch 92 : MAPE = 4.949219559395079\n",
            "Epoch 93 : MAPE = 4.949219588196334\n",
            "Epoch 94 : MAPE = 4.949219616997515\n",
            "Epoch 95 : MAPE = 4.94921964579863\n",
            "Epoch 96 : MAPE = 4.949219674599672\n",
            "Epoch 97 : MAPE = 4.949219703400646\n",
            "Epoch 98 : MAPE = 4.949219732201552\n",
            "Epoch 99 : MAPE = 4.949219761002386\n",
            "Epoch 100 : MAPE = 4.94921978980315\n",
            "Epoch 101 : MAPE = 4.949219818603845\n",
            "Epoch 102 : MAPE = 4.94921984740447\n",
            "Epoch 103 : MAPE = 4.949219876205024\n",
            "Epoch 104 : MAPE = 4.949219905005511\n",
            "Epoch 105 : MAPE = 4.949219933805925\n",
            "Epoch 106 : MAPE = 4.949219962606271\n",
            "Epoch 107 : MAPE = 4.949219991406548\n",
            "Epoch 108 : MAPE = 4.949220020206755\n",
            "Epoch 109 : MAPE = 4.949220049006891\n",
            "Epoch 110 : MAPE = 4.949220077806958\n",
            "Epoch 111 : MAPE = 4.949220106606955\n",
            "Epoch 112 : MAPE = 4.949220135406881\n",
            "Epoch 113 : MAPE = 4.94922016420674\n",
            "Epoch 114 : MAPE = 4.949220193006527\n",
            "Epoch 115 : MAPE = 4.949220221806244\n",
            "Epoch 116 : MAPE = 4.949220250605895\n",
            "Epoch 117 : MAPE = 4.949220279405471\n",
            "Epoch 118 : MAPE = 4.949220308204978\n",
            "Epoch 119 : MAPE = 4.9492203370044185\n",
            "Epoch 120 : MAPE = 4.949220365803787\n",
            "Epoch 121 : MAPE = 4.949220394603087\n",
            "Epoch 122 : MAPE = 4.949220423402316\n",
            "Epoch 123 : MAPE = 4.949220452201477\n",
            "Epoch 124 : MAPE = 4.949220481000566\n",
            "Epoch 125 : MAPE = 4.949220509799587\n",
            "Epoch 126 : MAPE = 4.949220538598538\n",
            "Epoch 127 : MAPE = 4.949220567397416\n",
            "Epoch 128 : MAPE = 4.949220596196229\n",
            "Epoch 129 : MAPE = 4.949220624994971\n",
            "Epoch 130 : MAPE = 4.949220653793642\n",
            "Epoch 131 : MAPE = 4.949220682592244\n",
            "Epoch 132 : MAPE = 4.949220711390774\n",
            "Epoch 133 : MAPE = 4.9492207401892365\n",
            "Epoch 134 : MAPE = 4.9492207689876295\n",
            "Epoch 135 : MAPE = 4.9492207977859515\n",
            "Epoch 136 : MAPE = 4.949220826584204\n",
            "Epoch 137 : MAPE = 4.9492208553823875\n",
            "Epoch 138 : MAPE = 4.9492208841805\n",
            "Epoch 139 : MAPE = 4.949220912978544\n",
            "Epoch 140 : MAPE = 4.949220941776518\n",
            "Epoch 141 : MAPE = 4.949220970574421\n",
            "Epoch 142 : MAPE = 4.949220999372255\n",
            "Epoch 143 : MAPE = 4.949221028170021\n",
            "Epoch 144 : MAPE = 4.949221056967714\n",
            "Epoch 145 : MAPE = 4.9492210857653385\n",
            "Epoch 146 : MAPE = 4.949221114562895\n",
            "Epoch 147 : MAPE = 4.949221143360381\n",
            "Epoch 148 : MAPE = 4.949221172157795\n",
            "Epoch 149 : MAPE = 4.949221200955142\n",
            "Epoch 150 : MAPE = 4.949221229752416\n",
            "Epoch 151 : MAPE = 4.949221258549622\n",
            "Epoch 152 : MAPE = 4.949221287346759\n",
            "Epoch 153 : MAPE = 4.949221316143825\n",
            "Epoch 154 : MAPE = 4.949221344940823\n",
            "Epoch 155 : MAPE = 4.9492213737377515\n",
            "Epoch 156 : MAPE = 4.949221402534608\n",
            "Epoch 157 : MAPE = 4.949221431331397\n",
            "Epoch 158 : MAPE = 4.949221460128113\n",
            "Epoch 159 : MAPE = 4.949221488924761\n",
            "Epoch 160 : MAPE = 4.949221517721339\n",
            "Epoch 161 : MAPE = 4.9492215465178475\n",
            "Epoch 162 : MAPE = 4.949221575314288\n",
            "Epoch 163 : MAPE = 4.949221604110659\n",
            "Epoch 164 : MAPE = 4.949221632906955\n",
            "Epoch 165 : MAPE = 4.949221661703184\n",
            "Epoch 166 : MAPE = 4.949221690499344\n",
            "Epoch 167 : MAPE = 4.949221719295434\n",
            "Epoch 168 : MAPE = 4.949221748091456\n",
            "Epoch 169 : MAPE = 4.949221776887405\n",
            "Epoch 170 : MAPE = 4.949221805683286\n",
            "Epoch 171 : MAPE = 4.949221834479096\n",
            "Epoch 172 : MAPE = 4.94922186327484\n",
            "Epoch 173 : MAPE = 4.949221892070509\n",
            "Epoch 174 : MAPE = 4.949221920866111\n",
            "Epoch 175 : MAPE = 4.949221949661643\n",
            "Epoch 176 : MAPE = 4.949221978457105\n",
            "Epoch 177 : MAPE = 4.949222007252498\n",
            "Epoch 178 : MAPE = 4.94922203604782\n",
            "Epoch 179 : MAPE = 4.949222064843073\n",
            "Epoch 180 : MAPE = 4.949222093638255\n",
            "Epoch 181 : MAPE = 4.94922212243337\n",
            "Epoch 182 : MAPE = 4.949222151228412\n",
            "Epoch 183 : MAPE = 4.949222180023387\n",
            "Epoch 184 : MAPE = 4.94922220881829\n",
            "Epoch 185 : MAPE = 4.949222237613125\n",
            "Epoch 186 : MAPE = 4.949222266407889\n",
            "Epoch 187 : MAPE = 4.949222295202584\n",
            "Epoch 188 : MAPE = 4.949222323997208\n",
            "Epoch 189 : MAPE = 4.9492223527917645\n",
            "Epoch 190 : MAPE = 4.949222381586249\n",
            "Epoch 191 : MAPE = 4.949222410380667\n",
            "Epoch 192 : MAPE = 4.9492224391750135\n",
            "Epoch 193 : MAPE = 4.949222467969289\n",
            "Epoch 194 : MAPE = 4.949222496763495\n",
            "Epoch 195 : MAPE = 4.94922252555763\n",
            "Epoch 196 : MAPE = 4.949222554351697\n",
            "Epoch 197 : MAPE = 4.949222583145694\n",
            "Epoch 198 : MAPE = 4.949222611939621\n",
            "Epoch 199 : MAPE = 4.94922264073348\n",
            "Epoch 200 : MAPE = 4.949222669527267\n",
            "\n",
            "---------- MAPE ----------\n",
            "MAPE of validation set : 4.9843450987673075\n",
            "\n",
            "---------- Coefficient ----------\n",
            "0.9486378390461381 50.00169355057738 "
          ]
        }
      ],
      "source": [
        "train, validate = split_data(preprocess_data(training_datalist, 2))\n",
        "coefficient = gradient_descent(200, 0.0000005, train)\n",
        "output_datalist = make_prediction(testing_datalist, coefficient)\n",
        "\n",
        "print('\\n' + '-' * 10 + ' MAPE ' + '-' * 10)\n",
        "print(validation(validate, coefficient))\n",
        "print('\\n' + '-' * 10 + ' Coefficient ' + '-' * 10)\n",
        "\n",
        "for coe in coefficient[::-1]:\n",
        "    print(coe[0], end = ' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1DpV_HcYFpl"
      },
      "source": [
        "### *Write the Output File*\n",
        "\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "**Write the coefficient update to csv**\n",
        "> Format: 'w0', 'w1', ..., 'wn'\n",
        ">*   The number of columns is based on your number of coefficient\n",
        ">*   The number of row is based on your number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 392,
      "metadata": {
        "id": "NLSHgpDvDXNI"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "\twriter = csv.writer(csvfile)\n",
        "\tfor row in output_datalist:\n",
        "\t\twriter.writerow(row)\n",
        "\n",
        "with open(coefficient_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "\twriter = csv.writer(csvfile)\n",
        "\tfor row in coefficient_output:\n",
        "\t\twriter.writerow(row.flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4408qg4xMQ"
      },
      "source": [
        "# **2. Advanced Part (40%)**\n",
        "In the second part, you need to implement the regression in a different way than the basic part to help your predictions of multiple patients SBP.\n",
        "\n",
        "You can choose **either** Matrix Inversion or Gradient Descent method.\n",
        "\n",
        "The training data will be in **hw1_advanced_training.csv** and the testing data will be in **hw1_advanced_testing.csv**.\n",
        "\n",
        "Output your prediction in **hw1_advanced.csv**\n",
        "\n",
        "Notice:\n",
        "> You cannot import any other package other than those given\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Input the training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {
        "id": "v66HUClZcxaE"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_advanced_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_advanced_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_advanced.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 220 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. read the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 394,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_data() -> (list, list, np.array):\n",
        "    train_id = []\n",
        "    train_time = []\n",
        "    train_data = []\n",
        "\n",
        "    training_data = []\n",
        "    with open(training_dataroot, newline='') as csvfile:\n",
        "        training_data = list(csv.reader(csvfile))[1 : ]\n",
        "\n",
        "    train_id = [int(data[0]) for data in training_data]\n",
        "\n",
        "    train_data = np.array([data[2 : ] for data in training_data])\n",
        "    train_data[train_data == ''] = np.nan\n",
        "    train_data = train_data.astype(float)\n",
        "\n",
        "    for data in training_data:\n",
        "        time_list = data[1].split(' ')\n",
        "        days_ago = int(time_list[0])\n",
        "\n",
        "        time = time_list[2].split(':')\n",
        "        hour, minute = int(time[0]), int(time[1])\n",
        "        train_time.append([days_ago, hour, minute])\n",
        "\n",
        "    return train_id, train_time, train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. preprocess the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 395,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_incomplete_data(train_id: list, train_time: list, train_data: np.array) -> (list, list, np.array):\n",
        "    i = 0; train_nums = len(train_data)\n",
        "    while i < train_nums:\n",
        "        if np.sum(np.isnan(train_data[i])) >= 2:\n",
        "            train_data = np.delete(train_data, i, 0)\n",
        "            train_id.pop(i)\n",
        "            train_time.pop(i)\n",
        "            train_nums -= 1\n",
        "            i -= 1\n",
        "\n",
        "        if np.isnan(train_data[i][0]):\n",
        "            train_data[i][0] = train_data[i - 1][0]\n",
        "        elif np.isnan(train_data[i][1]):\n",
        "            train_data[i][1] = train_data[i - 1][1]\n",
        "        elif np.isnan(train_data[i][2]):\n",
        "            train_data[i][2] = train_data[i - 1][2]\n",
        "        elif np.isnan(train_data[i][3]):\n",
        "            train_data[i][3] = train_data[i - 1][3]\n",
        "\n",
        "        i += 1\n",
        "    return train_id, train_time, train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 396,
      "metadata": {},
      "outputs": [],
      "source": [
        "def divide_data(train_id: list, train_time: list, train_data: np.array) -> list:\n",
        "    ## We first divide the data into group by id\n",
        "    last_id = train_id[0]; group = []; tem_group = []\n",
        "    for i, id in enumerate(train_id):\n",
        "        tem_group.append(train_data[i])\n",
        "        if id != last_id:\n",
        "            tem_group.pop()\n",
        "            group.append(tem_group)\n",
        "            tem_group = [train_data[i]]\n",
        "            last_id = id\n",
        "    group.append(tem_group) # list-list-ndarray\n",
        "\n",
        "    ## Then we divide the data into group by time\n",
        "    new_group = []\n",
        "    for people in group:\n",
        "        morning, noon, afternoon, evening, night = [], [], [], [], []\n",
        "\n",
        "        for i in range(len(people)):\n",
        "            clock = train_time.pop(0)[1]\n",
        "            \n",
        "            if clock >= 7 and clock < 10:\n",
        "                morning.append([*train_data[i]])\n",
        "            elif clock >= 10 and clock < 14:\n",
        "                noon.append([*train_data[i]])\n",
        "            elif clock >= 14 and clock < 17:\n",
        "                afternoon.append([*train_data[i]])\n",
        "            elif clock >= 17 and clock < 21:\n",
        "                evening.append([*train_data[i]])\n",
        "            else:\n",
        "                night.append([*train_data[i]])\n",
        "\n",
        "        new_group.append([morning, noon, afternoon, evening, night]) # list-list-list-ndarray\n",
        "    return new_group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 397,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_outliers(group: list, width: float) -> list:\n",
        "    new_group = []\n",
        "\n",
        "    for people in group:\n",
        "        new_people = []\n",
        "\n",
        "        for time in people:\n",
        "            new_time = []\n",
        "\n",
        "            temperature = np.array([data[0] for data in time])\n",
        "            temperature_mean = np.mean(temperature); temperature_std = np.std(temperature)\n",
        "\n",
        "            heartrate = np.array([data[1] for data in time])\n",
        "            heartrate_mean = np.mean(heartrate); heartrate_std = np.std(heartrate)\n",
        "\n",
        "            resprate = np.array([data[2] for data in time])\n",
        "            resprate_mean = np.mean(resprate); resprate_std = np.std(resprate)\n",
        "\n",
        "            o2sat = np.array([data[3] for data in time])\n",
        "            o2sat_mean = np.mean(o2sat); o2sat_std = np.std(o2sat)\n",
        "\n",
        "            sbp = np.array([data[4] for data in time])\n",
        "            sbp_mean = np.mean(sbp); sbp_std = np.std(sbp)\n",
        "\n",
        "            temp_beyond_threshold = np.abs(temperature - temperature_mean) > width * temperature_std\n",
        "            heartrate_beyond_threshold = np.abs(heartrate - heartrate_mean) > width * heartrate_std\n",
        "            resprate_beyond_threshold = np.abs(resprate - resprate_mean) > width * resprate_std\n",
        "            o2sat_beyond_threshold = np.abs(o2sat - o2sat_mean) > width * o2sat_std\n",
        "            sbp_beyond_threshold = np.abs(sbp - sbp_mean) > width * sbp_std\n",
        "\n",
        "            for i in range(len(time)):\n",
        "                if temp_beyond_threshold[i] or heartrate_beyond_threshold[i] or resprate_beyond_threshold[i] or o2sat_beyond_threshold[i] or sbp_beyond_threshold[i]:\n",
        "                    continue\n",
        "                new_time.append(time[i])\n",
        "            new_people.append(time)\n",
        "        new_group.append(new_people)\n",
        "\n",
        "    return new_group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 398,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(train_id: np.array, train_time: np.array, train_data: np.array, width: float) -> list:\n",
        "    '''\n",
        "    Assume the return value called list, then:\n",
        "        list: Whole data after preprocessing \\n\n",
        "        list[i]: Whole Data of the i-th person \\n\n",
        "        list[i][j]: Whole Data of the i-th person in the j-th time period \\n\n",
        "        list[i][j][k]: k-th Data of the i-th person in the j-th time period \\n\n",
        "        list[i][j][k][0]: Temperature of the k-th data of the i-th person in the j-th time period\n",
        "    '''\n",
        "    # remove the data with more than 2 elements missing\n",
        "    train_id, train_time, train_data = remove_incomplete_data(train_id, train_time, train_data)\n",
        "\n",
        "    # remove outliers\n",
        "    ## We first divide the data into group by id and time\n",
        "    group = divide_data(train_id, train_time, train_data)\n",
        "\n",
        "    ## Then we remove the data in each group once it has any outlier and return\n",
        "    return remove_outliers(group, width), train_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 399,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_descent(y_ans: np.array, y_predict: np.array, x: np.array) -> np.array:\n",
        "    return (-2 * (y_ans - y_predict)).T.dot(x).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 400,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Adam(gradient, coefficient, eta, m_t, v_t, n) -> np.array:\n",
        "    beta_1 = 0.9; beta_2 = 0.9; epsilon = 1e-7\n",
        "    m_t = beta_1 * m_t + (1 - beta_1) * gradient\n",
        "    v_t = beta_2 * v_t + (1 - beta_2) * (gradient ** 2)\n",
        "\n",
        "    m_hat = m_t / (1 - beta_1 ** n)\n",
        "    v_hat = v_t / (1 - beta_2 ** n)\n",
        "\n",
        "    return coefficient - eta * m_hat / (np.sqrt(v_hat) + epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 401,
      "metadata": {},
      "outputs": [],
      "source": [
        "mape = 0\n",
        "def train(datalist: np.array, epoch: int, learning_rate: float, model: np.array) -> np.array:\n",
        "    # We first divide the data into train set and validate set\n",
        "    nums = len(datalist)\n",
        "    tem_train = datalist[ : int(nums * 0.9)]\n",
        "    train_set = np.array(tem_train)\n",
        "\n",
        "    tem_validate = datalist[int(nums * 0.9) : ]\n",
        "    validate_set = np.array(tem_validate)\n",
        "\n",
        "    # Then train the model\n",
        "    tem_x = []; tem_y = []\n",
        "    for data in train_set:\n",
        "        tem_x.append([1, *data[0 : -1]])\n",
        "        tem_y.append([data[-1]])\n",
        "    x = np.array(tem_x).astype(float)\n",
        "    y_ans = np.array(tem_y).astype(int)\n",
        "\n",
        "    m_t = np.zeros(model.shape); v_t = np.zeros(model.shape)\n",
        "    for e in range(epoch):\n",
        "        y_prediction = x.dot(model).astype(int)\n",
        "        gradient = gradient_descent(y_ans, y_prediction, x)\n",
        "        model = Adam(gradient, model, learning_rate, m_t, v_t, e + 1)\n",
        "        # print(f'Epoch {e + 1:2} : MAPE = {MAPE(y_ans, y_prediction)}, coefficient = {model.flatten()}')\n",
        "\n",
        "    # Finally, validate the model\n",
        "    tem_x = []; tem_y = []\n",
        "    for data in validate_set:\n",
        "        tem_x.append([1, *data[0 : -1]])\n",
        "        tem_y.append([data[-1]])\n",
        "    x = np.array(tem_x).astype(float)\n",
        "    y_ans = np.array(tem_y).astype(int)\n",
        "    y_prediction = x.dot(model).astype(int)\n",
        "    print(f'Validation MAPE = {MAPE(y_ans, y_prediction)}')\n",
        "\n",
        "    global mape\n",
        "    mape += MAPE(y_ans, y_prediction)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 402,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[11526383, 12923910, 14699420, 15437705, 15642911, 16298357, 17331999, 17593883, 18733920, 18791093, 19473413]\n",
            "Model 1 : Validation MAPE = 12.11309053101792\n",
            "Model 2 : Validation MAPE = 30.76485354839042\n",
            "Model 3 : Validation MAPE = 42.23760353923039\n",
            "Model 4 : Validation MAPE = 11.016786624730715\n",
            "Model 5 : Validation MAPE = 12.559946737293126\n",
            "Model 6 : Validation MAPE = 20.685563251646222\n",
            "Model 7 : Validation MAPE = 10.56848475243033\n",
            "Model 8 : Validation MAPE = 11.90696756222098\n",
            "Model 9 : Validation MAPE = 12.757585497250066\n",
            "Model 10 : Validation MAPE = 12.051199697742646\n",
            "Model 11 : Validation MAPE = 14.458638634491228\n",
            "17.374610943313094\n"
          ]
        }
      ],
      "source": [
        "people, model_id = preprocess(*read_data(), 2.25)\n",
        "tem = []; tem2 = []\n",
        "for time in people:\n",
        "    for datas in time:\n",
        "        for data in datas:\n",
        "            tem.append(data)\n",
        "\n",
        "    random.shuffle(tem)\n",
        "    tem2.append(tem)\n",
        "    tem = []\n",
        "people = tem2\n",
        "\n",
        "tem = [model_id[0]]\n",
        "for data in model_id:\n",
        "    if data != tem[-1]:\n",
        "        tem.append(int(data))\n",
        "print(tem)\n",
        "model_id = tem\n",
        "\n",
        "model_1 = np.array([[-10], [0.1], [1], [0.5], [0.8]]); print('Model 1 : ', end = '')\n",
        "model_1 = train(people[0], 500, 0.05, model_1)\n",
        "\n",
        "model_2 = np.array([[-10], [0.1], [1], [0.5], [0.8]]); print('Model 2 : ', end = '')\n",
        "model_2 = train(people[1], 500, 0.05, model_2)\n",
        "\n",
        "model_3 = np.array([[-10], [0.1], [1], [0.5], [0.8]]); print('Model 3 : ', end = '')\n",
        "model_3 = train(people[2], 500, 0.05, model_3)\n",
        "\n",
        "model_4 = np.array([[-10], [0.1], [1], [0.5], [0.8]]); print('Model 4 : ', end = '')\n",
        "model_4 = train(people[3], 500, 0.05, model_4)\n",
        "\n",
        "model_5 = np.array([[-5], [0.1], [0.4], [0.5], [1]]); print('Model 5 : ', end = '')\n",
        "model_5 = train(people[4], 500, 0.05, model_5)\n",
        "\n",
        "model_6 = np.array([[-10], [0.1], [1], [0.5], [0.8]]); print('Model 6 : ', end = '')\n",
        "model_6 = train(people[5], 500, 0.05, model_6)\n",
        "\n",
        "model_7 = np.array([[-10], [0.1], [1], [0.5], [0.8]]); print('Model 7 : ', end = '')\n",
        "model_7 = train(people[6], 500, 0.05, model_7)\n",
        "\n",
        "model_8 = np.array([[-10], [0.1], [1], [0.5], [0.8]]); print('Model 8 : ', end = '')\n",
        "model_8 = train(people[7], 500, 0.05, model_8)\n",
        "\n",
        "model_9 = np.array([[-30], [0.1], [0.8], [0.5], [0.8]]); print('Model 9 : ', end = '')\n",
        "model_9 = train(people[8], 500, 0.05, model_9)\n",
        "\n",
        "model_10 = np.array([[-10], [0.1], [1], [0.5], [0.8]]); print('Model 10 : ', end = '')\n",
        "model_10 = train(people[9], 500, 0.05, model_10)\n",
        "\n",
        "model_11 = np.array([[-10], [0.1], [1], [0.5], [0.8]]); print('Model 11 : ', end = '')\n",
        "model_11 = train(people[10], 500, 0.05, model_11)\n",
        "\n",
        "print(mape / 11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 403,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(train_set: np.array, epoch: int, learning_rate: float, model: np.array) -> np.array:\n",
        "    '''\n",
        "    What I want to do: https://chat.openai.com/share/d4fa50cc-43d6-4651-93c7-9f42a8c49a3a\n",
        "    '''\n",
        "    eta = learning_rate\n",
        "    batch_nums = len(train_set)\n",
        "\n",
        "    plot_epoch = []; plot_train_MAPE = []; plot_validate_MAPE = []\n",
        "\n",
        "    MAPE_mean = 0\n",
        "    m_t = np.zeros((len(train_set[0][0]), 1)); v_t = np.zeros((len(train_set[0][0]), 1))\n",
        "    for e in range(epoch):\n",
        "        ith_validate = 0\n",
        "\n",
        "        validate_set = []; clean_train_set = []\n",
        "        for i ,batch in enumerate(train_set):\n",
        "            if i % batch_nums == ith_validate:\n",
        "                validate_set = batch\n",
        "                continue\n",
        "            \n",
        "            for datas in batch:\n",
        "                x = np.array([[1, *datas[ : -1]]]).astype(float)\n",
        "                y_ans = np.array([[datas[-1]]]).astype(float)\n",
        "                y_predict = x.dot(model).astype(int)\n",
        "\n",
        "                gradient = gradient_descent(y_ans, y_predict, x)\n",
        "                model = Adam(gradient, model, eta, m_t, v_t, i)\n",
        "\n",
        "            clean_train_set += batch\n",
        "            \n",
        "        validate_x = np.array([[1, *data[ : -1]] for data in validate_set]).astype(float)\n",
        "        validate_y = np.array([[data[-1]] for data in validate_set]).astype(float)\n",
        "        validate_y_prediction = validate_x.dot(model).astype(int)\n",
        "        MAPE_mean += MAPE(validate_y, validate_y_prediction)\n",
        "\n",
        "        clean_train_x = np.array([[1, *data[ : -1]] for data in clean_train_set]).astype(float)\n",
        "        clean_train_y = np.array([[data[-1]] for data in clean_train_set]).astype(float)\n",
        "        clean_train_y_prediction = clean_train_x.dot(model).astype(int)\n",
        "\n",
        "        plot_epoch.append(e + 1)\n",
        "        plot_validate_MAPE.append(MAPE(validate_y, validate_y_prediction))\n",
        "        plot_train_MAPE.append(MAPE(clean_train_y, clean_train_y_prediction))\n",
        "        \n",
        "    plt.plot(plot_epoch, plot_train_MAPE, 'blue')\n",
        "    plt.plot(plot_epoch, plot_validate_MAPE, 'red')\n",
        "    plt.xlabel('Epoch'); plt.ylabel('MAPE')\n",
        "    plt.show()\n",
        "\n",
        "    print(f'MAPE = {MAPE_mean / epoch}')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 404,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_set = preprocess(*read_data(), 2)\n",
        "\n",
        "# people_nums = len(train_set)\n",
        "# morning_train_set = [train_set[i][0] for i in range(people_nums)]\n",
        "# noon_train_set = [train_set[i][1] for i in range(people_nums)]\n",
        "# afternoon_train_set = [train_set[i][2] for i in range(people_nums)]\n",
        "# evening_train_set = [train_set[i][3] for i in range(people_nums)]\n",
        "# night_train_set = [train_set[i][4] for i in range(people_nums)]\n",
        "\n",
        "# a = []\n",
        "# for people in morning_train_set:\n",
        "#     for data in people:\n",
        "#         a.append(data)\n",
        "\n",
        "# nums = len(a)\n",
        "# print(nums)\n",
        "# morning_model = np.array([[5], [-0.2], [1], [0.25], [1]]).astype(float) # const, temperature, heartrate, resprate, o2sat\n",
        "# morning_model = train(morning_train_set, len(morning_train_set) * 25, 0.00005, morning_model)\n",
        "# print(*morning_model)\n",
        "# print('-' * 80)\n",
        "\n",
        "# noon_model = np.array([[5], [-0.2], [1], [0.25], [1]]).astype(float)\n",
        "# noon_model = train(noon_train_set, len(noon_train_set) * 20, 0.001, noon_model)\n",
        "# print(*noon_model)\n",
        "# print('-' * 80)\n",
        "\n",
        "# a = []\n",
        "# for people in afternoon_train_set:\n",
        "#     for data in people:\n",
        "#         a.append(data)\n",
        "\n",
        "# nums = len(a)\n",
        "# print(nums)\n",
        "# b = []; c = []\n",
        "# for i, data in enumerate(a):\n",
        "#     if i % (len(a) / 11) == 0 and i != 0:\n",
        "#         b.append(c)\n",
        "#         c = []\n",
        "#         continue\n",
        "#     c.append(data)\n",
        "\n",
        "# afternoon_train_set = b\n",
        "# afternoon_model = np.array([[20], [0.4], [0.45], [1.11], [0.3]]).astype(float)\n",
        "# afternoon_model = train(afternoon_train_set, len(afternoon_train_set) * 20, 0.00001, afternoon_model)\n",
        "# print(*afternoon_model)\n",
        "# print('-' * 80)\n",
        "\n",
        "# evening_model = np.array([[5], [-0.2], [1], [0.25], [1]]).astype(float)\n",
        "# evening_model = train(evening_train_set, len(evening_train_set) * 20, 0.0001, evening_model)\n",
        "# print(*evening_model)\n",
        "# print('-' * 80)\n",
        "\n",
        "# night_model = np.array([[5], [-0.2], [1], [0.25], [1]]).astype(float)\n",
        "# night_model = train(night_train_set, len(night_train_set) * 20, 0.001, night_model)\n",
        "# print(*night_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 405,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_2(datalist, epoch, learning_rate, model):\n",
        "    # random.shuffle(datalist)\n",
        "    nums = len(datalist)\n",
        "    train_set = datalist[ : int(nums * 0.8)]\n",
        "    validate_set = datalist[int(nums * 0.8) : ]\n",
        "\n",
        "    eta = learning_rate\n",
        "    plot_epoch = []; plot_train_MAPE = []; plot_validate_MAPE = []\n",
        "\n",
        "    m_t = np.zeros((len(train_set[0]), 1)); v_t = np.zeros((len(train_set[0]), 1))\n",
        "    for i, data in enumerate(train_set):\n",
        "        x = np.array([[1, *data[ : -1]]]).astype(float)\n",
        "        y_ans = np.array([[data[-1]]]).astype(float)\n",
        "        y_predict = x.dot(model).astype(int)\n",
        "\n",
        "        gradient = gradient_descent(y_ans, y_predict, x)\n",
        "        model = Adam(gradient, model, eta, m_t, v_t, i + 1)\n",
        "\n",
        "        plot_epoch.append(i + 1)\n",
        "        plot_train_MAPE.append(MAPE(y_ans, y_predict))\n",
        "\n",
        "    validate_x = np.array([[1, *data[ : -1]] for data in validate_set]).astype(float)\n",
        "    validate_y = np.array([[data[-1]] for data in validate_set]).astype(float)\n",
        "    validate_y_prediction = validate_x.dot(model).astype(int)\n",
        "    print(f'MAPE = {MAPE(validate_y, validate_y_prediction)}')\n",
        "\n",
        "    plt.plot(plot_epoch, plot_train_MAPE, 'blue')\n",
        "    plt.xlabel('Epoch'); plt.ylabel('MAPE')\n",
        "    plt.show()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 406,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_set = preprocess(*read_data(), 2.5)\n",
        "\n",
        "# a = []\n",
        "# for time in train_set:\n",
        "#     for people in time:\n",
        "#         for data in people:\n",
        "#             a.append(data)\n",
        "\n",
        "# a_model = np.array([[5], [-0.2], [1], [0.8], [1]]).astype(float)\n",
        "# a_model = train_2(a, 100, 0.001, a_model)\n",
        "# print(a_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. make prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 407,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[11526383, 12923910, 14699420, 15437705, 15642911, 16298357, 17331999, 17593883, 18733920, 18791093, 19473413]\n",
            "220 220\n"
          ]
        }
      ],
      "source": [
        "testing_data = []\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "    testing_data = list(csv.reader(csvfile))[1 : ]\n",
        "\n",
        "test_id = [int(data[0]) for data in testing_data]\n",
        "\n",
        "test_data = np.array([data[2 : -1] for data in testing_data]).astype(float)\n",
        "x_list = []\n",
        "for datas in test_data:\n",
        "\tx = np.array([[1, *datas]]).astype(float)\n",
        "\tx_list.append(x)\n",
        "\n",
        "print(model_id)\n",
        "print(len(x_list), len(test_id))\n",
        "\n",
        "for i, id in enumerate(test_id):\n",
        "\tif id == model_id[0]:\n",
        "\t\toutput_datalist.append(x_list[i].dot(model_1).astype(int).flatten())\n",
        "\telif id == model_id[1]:\n",
        "\t\toutput_datalist.append(x_list[i].dot(model_2).astype(int).flatten())\n",
        "\telif id == model_id[2]:\n",
        "\t\toutput_datalist.append(x_list[i].dot(model_3).astype(int).flatten())\n",
        "\telif id == model_id[3]:\n",
        "\t\toutput_datalist.append(x_list[i].dot(model_4).astype(int).flatten())\n",
        "\telif id == model_id[4]:\n",
        "\t\toutput_datalist.append(x_list[i].dot(model_5).astype(int).flatten())\n",
        "\telif id == model_id[5]:\n",
        "\t\toutput_datalist.append(x_list[i].dot(model_6).astype(int).flatten())\n",
        "\telif id == model_id[6]:\n",
        "\t\toutput_datalist.append(x_list[i].dot(model_7).astype(int).flatten())\n",
        "\telif id == model_id[7]:\n",
        "\t\toutput_datalist.append(x_list[i].dot(model_8).astype(int).flatten())\n",
        "\telif id == model_id[8]:\n",
        "\t\toutput_datalist.append(x_list[i].dot(model_9).astype(int).flatten())\n",
        "\telif id == model_id[9]:\n",
        "\t\toutput_datalist.append(x_list[i].dot(model_10).astype(int).flatten())\n",
        "\telif id == model_id[10]:\n",
        "\t\toutput_datalist.append(x_list[i].dot(model_11).astype(int).flatten())\n",
        "\telse:\n",
        "\t\tprint(id)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output your Prediction\n",
        "\n",
        "> your filename should be **hw1_advanced.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 408,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "\twriter = csv.writer(csvfile)\n",
        "\tfor row in output_datalist:\n",
        "\t\twriter.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtgCJU7FPeJL"
      },
      "source": [
        "# Report *(5%)*\n",
        "\n",
        "Report should be submitted as a pdf file **hw1_report.pdf**\n",
        "\n",
        "*   Briefly describe the difficulty you encountered\n",
        "*   Summarize your work and your reflections\n",
        "*   No more than one page\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlEE53_MPf4W"
      },
      "source": [
        "# Save the Code File\n",
        "Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

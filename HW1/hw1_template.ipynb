{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Te27fi-0pP"
      },
      "source": [
        "# **HW1: Regression**\n",
        "In *assignment 1*, you need to finish:\n",
        "\n",
        "1.  Basic Part: Implement two regression models to predict the Systolic blood pressure (SBP) of a patient. You will need to implement **both Matrix Inversion and Gradient Descent**.\n",
        "\n",
        "\n",
        "> *   Step 1: Split Data\n",
        "> *   Step 2: Preprocess Data\n",
        "> *   Step 3: Implement Regression\n",
        "> *   Step 4: Make Prediction\n",
        "> *   Step 5: Train Model and Generate Result\n",
        "\n",
        "2.  Advanced Part: Implement one regression model to predict the SBP of multiple patients in a different way than the basic part. You can choose **either** of the two methods for this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wDdnos-4uUv"
      },
      "source": [
        "# **1. Basic Part (55%)**\n",
        "In the first part, you need to implement the regression to predict SBP from the given DBP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_EVqWlB-DTF"
      },
      "source": [
        "## 1.1 Matrix Inversion Method (25%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_mi.csv**\n",
        "*   Print your coefficient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCR7vk9BFkf"
      },
      "source": [
        "### *Import Packages*\n",
        "\n",
        "> Note: You **cannot** import any other package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "HL5XjqFf4wSj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnWjrzi0dMPz"
      },
      "source": [
        "### *Global attributes*\n",
        "Define the global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "EWLDPOlHBbcK"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_basic_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_basic_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_basic_mi.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 3 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['subject_id', 'charttime', 'sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsFC-cvqIcYK"
      },
      "source": [
        "You can add your own global attributes here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "OUbS2BEgcut6"
      },
      "outputs": [],
      "source": [
        "training_dataset_1 = []     # The first 80% of training_datalist\n",
        "validation_dataset_1 = []   # The last 20% of training_datalist\n",
        "\n",
        "training_dataset_2 = []     # Randomly selected 80% of training_datalist\n",
        "validation_dataset_2 = []   # The rest of training_datalist\n",
        "\n",
        "training_dataset_3 = []     # Randomly selected 70% of training_datalist\n",
        "validation_dataset_3 = []   # The rest of training_datalist\n",
        "\n",
        "training_dataset_4 = []     # Randomly selected 60% of training_datalist\n",
        "validation_dataset_4 = []   # The rest of training_datalist\n",
        "\n",
        "training_data_num = 0\n",
        "\n",
        "def MAPE(y_ans, y_prediction):\n",
        "    return np.mean(np.abs((y_ans - y_prediction) / y_ans)) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUoRFoQjBW5S"
      },
      "source": [
        "### *Load the Input File*\n",
        "First, load the basic input file **hw1_basic_training.csv** and **hw1_basic_testing.csv**\n",
        "\n",
        "Input data would be stored in *training_datalist* and *testing_datalist*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "dekR1KnqBtI6"
      },
      "outputs": [],
      "source": [
        "# Read input csv to datalist\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "\tglobal training_datalist, training_data_num\n",
        "\t\n",
        "\ttraining_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\ttraining_datalist = np.delete(training_datalist, 0, 0)\n",
        "\ttraining_data_num = training_datalist.shape[0]\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "\tglobal testing_datalist\n",
        "\n",
        "\ttesting_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\ttesting_datalist = np.delete(testing_datalist, 0, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kYPuikLCFx4"
      },
      "source": [
        "### *Implement the Regression Model*\n",
        "\n",
        "> Note: It is recommended to use the functions we defined, you can also define your own functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWwdx06JNEYs"
      },
      "source": [
        "#### Step 1: Split Data\n",
        "Split data in *training_datalist* into training dataset and validation dataset\n",
        "* Validation dataset is used to validate your own model without the testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "USDciENcB-5F"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "    def RandomSplitData(propotion):\n",
        "        # We have to deepcopy the training_datalist since we don't want to change the original one\n",
        "        training_datalist_copy = training_datalist.copy()\n",
        "        \n",
        "        random_times = random.randint(1, 10)\n",
        "        for i in range(random_times):\n",
        "            # Since the copy is a numpy array, we have to use np.random.shuffle instead of random.shuffle\n",
        "            # Or it will have some repeated elements after shuffling\n",
        "            np.random.shuffle(training_datalist_copy)\n",
        "\n",
        "        return training_datalist_copy[ : propotion], training_datalist_copy[propotion : ]\n",
        "\n",
        "    # We have to minus 1 since the first row is the name of each column\n",
        "    global training_data_num\n",
        "    propotion_80 = int(training_data_num * 0.8 + 0.5)   # 0.5 is for rounding\n",
        "    propotion_70 = int(training_data_num * 0.7 + 0.5)\n",
        "    propotion_60 = int(training_data_num * 0.6 + 0.5)\n",
        "    \n",
        "    global training_datalist\n",
        "    global training_dataset_1, validation_dataset_1\n",
        "    global training_dataset_2, validation_dataset_2\n",
        "    global training_dataset_3, validation_dataset_3 \n",
        "    global training_dataset_4, validation_dataset_4\n",
        "\n",
        "    training_dataset_1 = training_datalist[0 : propotion_80 ]\n",
        "    validation_dataset_1 = training_datalist[propotion_80 : ]\n",
        "    training_dataset_2, validation_dataset_2 = RandomSplitData(propotion_80)\n",
        "    training_dataset_3, validation_dataset_3 = RandomSplitData(propotion_70)\n",
        "    training_dataset_4, validation_dataset_4 = RandomSplitData(propotion_60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-3Qln4aNgVy"
      },
      "source": [
        "#### Step 2: Preprocess Data\n",
        "Handle the unreasonable data\n",
        "> Hint: Outlier and missing data can be handled by removing the data or adding the values with the help of statistics  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "XXvW1n_5NkQ5"
      },
      "outputs": [],
      "source": [
        "def PreprocessData():\n",
        "    '''\n",
        "    # ref\n",
        "    1. https://andy6804tw.github.io/2021/04/02/python-outliers-clean/ \\n\n",
        "    2. https://chat.openai.com/share/76e7e9a6-7e42-4cdc-a2bb-72af60082814\n",
        "\n",
        "    By searching some technique, I decided to remove the outliers by using IQR\n",
        "    Since I roughly check the training data and found that the outliers are not too much\n",
        "    If just one of the number in the row is outlier, I will modify its value based on another number in the same row\n",
        "    '''\n",
        "    \n",
        "    global training_datalist, training_data_num\n",
        "    dbp_list = np.array([float(training_datalist[i][0]) for i in range(training_data_num)])\n",
        "    sbp_list = np.array([float(training_datalist[i][1]) for i in range(training_data_num)])\n",
        "\n",
        "    dbp_IQR = np.percentile(dbp_list, 75) - np.percentile(dbp_list, 25)\n",
        "    sbp_IQR = np.percentile(sbp_list, 75) - np.percentile(sbp_list, 25)\n",
        "\n",
        "    dbp_upper = np.percentile(dbp_list, 75) + 1.5 * dbp_IQR\n",
        "    dbp_lower = np.percentile(dbp_list, 25) - 1.5 * dbp_IQR\n",
        "    sbp_upper = np.percentile(sbp_list, 75) + 1.5 * sbp_IQR\n",
        "    sbp_lower = np.percentile(sbp_list, 25) - 1.5 * sbp_IQR\n",
        "\n",
        "    tem_list = list()\n",
        "    for i in range(training_data_num):\n",
        "        diff_with_mean_dbp = (dbp_list[i] - np.mean(dbp_list)) / dbp_IQR\n",
        "        diff_with_mean_sbp = (sbp_list[i] - np.mean(sbp_list)) / sbp_IQR\n",
        "        is_outlier_dbp = bool(dbp_list[i] > dbp_upper or dbp_list[i] < dbp_lower)\n",
        "        is_outlier_sbp = bool(sbp_list[i] > sbp_upper or sbp_list[i] < sbp_lower)\n",
        "\n",
        "        if is_outlier_dbp and is_outlier_sbp:\n",
        "            continue\n",
        "        elif is_outlier_dbp:\n",
        "            dbp_list[i] = int(np.mean(dbp_list) + diff_with_mean_sbp * dbp_IQR + 0.5)\n",
        "        elif is_outlier_sbp:\n",
        "            sbp_list[i] = int(np.mean(sbp_list) + diff_with_mean_dbp * sbp_IQR + 0.5)\n",
        "\n",
        "        # if is_outlier_dbp or is_outlier_sbp: \n",
        "        #     continue\n",
        "        tem_list.append([dbp_list[i], sbp_list[i]])\n",
        "\n",
        "    training_datalist = np.array(tem_list)\n",
        "    training_data_num = training_datalist.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDLpJmQUN3V6"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Matrix Inversion to finish this part\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "Tx9n1_23N8C0"
      },
      "outputs": [],
      "source": [
        "def MatrixInversion():\n",
        "    global training_dataset_1\n",
        "    global training_dataset_2\n",
        "    global training_dataset_3\n",
        "    global training_dataset_4\n",
        "\n",
        "    training_dataset_list = []\n",
        "    training_dataset_list.append(training_dataset_1)\n",
        "    training_dataset_list.append(training_dataset_2)\n",
        "    training_dataset_list.append(training_dataset_3)\n",
        "    training_dataset_list.append(training_dataset_4)\n",
        "\n",
        "    # each element contains [phi, phi_T, y]\n",
        "    container = [[\n",
        "        np.array([[1, float(training_dataset_list[i][j][0])] for j in range(len(training_dataset_list[i]))]),\n",
        "        np.array([[1, float(training_dataset_list[i][j][0])] for j in range(len(training_dataset_list[i]))]).T,\n",
        "        np.array([float(training_dataset_list[i][j][1]) for j in range(len(training_dataset_list[i]))])\n",
        "    ] for i in range(len(training_dataset_list))]\n",
        "\n",
        "    w = [ np.dot( np.dot( np.linalg.inv(np.dot(container[i][1], container[i][0])), container[i][1]), container[i][2]).tolist() for i in range(len(container)) ]\n",
        "    return [sum(row[0] for row in w) / len(w), sum(row[1] for row in w) / len(w)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NxRNFwyN8xd"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "Make prediction of testing dataset and store the value in *output_datalist*\n",
        "The final *output_datalist* should look something like this \n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "EKlDIC2-N_lk"
      },
      "outputs": [],
      "source": [
        "def MakePrediction(coefficients):\n",
        "    global testing_datalist, output_datalist\n",
        "\n",
        "    output_datalist = np.dot(\n",
        "        np.array([ [1, int(testing_datalist[i][0])] for i in range(len(testing_datalist)) ]),\n",
        "        np.array([[coefficients[0]], [coefficients[1]]])\n",
        "    )\n",
        "\n",
        "    # testing_dbp = np.array([ [1, int(testing_datalist[i][0])] for i in range(len(testing_datalist)) ])\n",
        "    # coe = np.array([[coefficients[0]], [coefficients[1]]])\n",
        "    # result = np.dot(testing_dbp, coe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCd0Z6izOCwq"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "iCL92EPKOFIn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "after preprocessing : 373\n",
            "first training list num : 298\n",
            "first validation list num : 75\n",
            "\n",
            "second training list num : 298\n",
            "second validation list num : 75\n",
            "\n",
            "third training list num : 261\n",
            "third validation list num : 112\n",
            "\n",
            "fourth training list num : 224\n",
            "fourth validation list num : 149\n",
            "\n",
            "------------------------------\n",
            "0.9769529294792862 48.618843763005756\n",
            "\n",
            "------------------------------\n",
            "MAPE for training set 1 : 12.979509241449186\n",
            "MAPE for training set 2 : 12.18126125319156\n",
            "MAPE for training set 3 : 13.023998568049239\n",
            "MAPE for training set 4 : 11.97945093598516\n"
          ]
        }
      ],
      "source": [
        "def Validation(datalist, coefficients):\n",
        "    prediction = np.dot(\n",
        "        np.array([ [1, int(datalist[i][0])] for i in range(len(datalist)) ]),\n",
        "        np.array([[coefficients[0]], [coefficients[1]]])\n",
        "    )\n",
        "\n",
        "    return MAPE(np.array([float(datalist[i][1]) for i in range(len(datalist))]), prediction)\n",
        "    \n",
        "\n",
        "PreprocessData()\n",
        "SplitData()\n",
        "print(f'after preprocessing : {training_datalist.shape[0]}')\n",
        "print(f'first training list num : {training_dataset_1.shape[0]}')\n",
        "print(f'first validation list num : {validation_dataset_1.shape[0]}\\n')\n",
        "print(f'second training list num : {training_dataset_2.shape[0]}')\n",
        "print(f'second validation list num : {validation_dataset_2.shape[0]}\\n')\n",
        "print(f'third training list num : {training_dataset_3.shape[0]}')\n",
        "print(f'third validation list num : {validation_dataset_3.shape[0]}\\n')\n",
        "print(f'fourth training list num : {training_dataset_4.shape[0]}')\n",
        "print(f'fourth validation list num : {validation_dataset_4.shape[0]}')\n",
        "print('\\n' + '-' * 30)\n",
        "\n",
        "coefficients = MatrixInversion()\n",
        "print(coefficients[1], coefficients[0])\n",
        "print('\\n' + '-' * 30)\n",
        "print(f'MAPE for training set 1 : {Validation(validation_dataset_1, coefficients)}')\n",
        "print(f'MAPE for training set 2 : {Validation(validation_dataset_2, coefficients)}')\n",
        "print(f'MAPE for training set 3 : {Validation(validation_dataset_3, coefficients)}')\n",
        "print(f'MAPE for training set 4 : {Validation(validation_dataset_4, coefficients)}')\n",
        "\n",
        "# MakePrediction(coefficients)\n",
        "# print(output_datalist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8Jhd8wAOk3D"
      },
      "source": [
        "### *Write the Output File*\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "tYQVYLlKOtDB"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "\twriter = csv.writer(csvfile)\n",
        "\tfor row in output_datalist:\n",
        "\t\twriter.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J3WOhglA9ML"
      },
      "source": [
        "## 1.2 Gradient Descent Method (30%)\n",
        "\n",
        "\n",
        "*   Save the prediction result in a csv file **hw1_basic_gd.csv**\n",
        "*   Output your coefficient update in a csv file **hw1_basic_coefficient.csv**\n",
        "*   Print your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkMqa_xjXhEv"
      },
      "source": [
        "### *Global attributes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "wNZtRWUeXpEu"
      },
      "outputs": [],
      "source": [
        "output_dataroot = 'hw1_basic_gd.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "coefficient_output_dataroot = 'hw1_basic_coefficient.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 20 * 3 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['subject_id', 'charttime', 'sbp']\n",
        "\n",
        "coefficient_output = [] # Your coefficient update during gradient descent\n",
        "                   # Should be a (number of iterations * number_of coefficient) matrix\n",
        "                   # The format of each row should be ['w0', 'w1', ...., 'wn']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5DeHxdLdai3"
      },
      "source": [
        "Your own global attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "_2IO5tYSdaFd"
      },
      "outputs": [],
      "source": [
        "# We reopen the file since we have modified the data in the previous part\n",
        "with open(training_dataroot, newline='') as csvfile:\n",
        "\tglobal training_datalist, training_data_num\n",
        "\t\n",
        "\ttraining_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\ttraining_datalist = np.delete(training_datalist, 0, 0)\n",
        "\ttraining_data_num = training_datalist.shape[0]\n",
        "\n",
        "with open(testing_dataroot, newline='') as csvfile:\n",
        "\tglobal testing_datalist\n",
        "\n",
        "\ttesting_datalist = np.array(list(csv.reader(csvfile)))\n",
        "\ttesting_datalist = np.delete(testing_datalist, 0, 0)\n",
        "\n",
        "training_list_1 = []\n",
        "validation_list_1 = []\n",
        "\n",
        "training_list_2 = []\n",
        "validation_list_2 = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVBLT1aqXuW0"
      },
      "source": [
        "### *Implement the Regression Model*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPWpcOnXhCZ"
      },
      "source": [
        "#### Step 1: Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "1PEf_qGvYHu0"
      },
      "outputs": [],
      "source": [
        "def SplitData():\n",
        "    global training_datalist\n",
        "    global training_list_1, validation_list_1\n",
        "    global training_list_2, validation_list_2\n",
        "    \n",
        "    _80_percent = int(training_data_num * 0.8 + 0.5)\n",
        "    _90_percent = int(training_data_num * 0.9 + 0.5)\n",
        "\n",
        "    training_list_1 = training_datalist[ : _80_percent]\n",
        "    validation_list_1 = training_datalist[_80_percent : ]\n",
        "\n",
        "    training_list_2 = training_datalist[ : _90_percent]\n",
        "    validation_list_2 = training_datalist[_90_percent : ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpSoPDPKX56w"
      },
      "source": [
        "#### Step 2: Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "uLTXOWRwYHiS"
      },
      "outputs": [],
      "source": [
        "def PreprocessData():\n",
        "    global training_datalist\n",
        "\n",
        "    training_nums = len(training_datalist)\n",
        "    dbp_list = np.array([float(training_datalist[i][0]) for i in range(training_nums)])\n",
        "    sbp_list = np.array([float(training_datalist[i][1]) for i in range(training_nums)])\n",
        "\n",
        "    dbp_std = np.std(dbp_list)\n",
        "    dbp_mean = np.mean(dbp_list)\n",
        "    sbp_std = np.std(sbp_list)\n",
        "    sbp_mean = np.mean(sbp_list)\n",
        "\n",
        "    tem = [\n",
        "        [dbp_list[i], sbp_list[i]] \n",
        "        for i in range(training_nums) if (dbp_list[i] < dbp_mean + 2 * dbp_std and dbp_list[i] > dbp_mean - 2 * dbp_std) and (sbp_list[i] < sbp_mean + 2 * sbp_std and sbp_list[i] > sbp_mean - 2 * sbp_std)\n",
        "    ]\n",
        "    print(len(tem))\n",
        "    training_datalist = np.array(tem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_y82gXX6a-"
      },
      "source": [
        "#### Step 3: Implement Regression\n",
        "> use Gradient Descent to finish this part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "-635Ee00YHTE"
      },
      "outputs": [],
      "source": [
        "def GradientDescent(learning_rate):\n",
        "    def gradient_of_loss(y_ans, y_prediction, x):\n",
        "        return (-2 * (y_ans - y_prediction)).dot(x)\n",
        "    \n",
        "    global training_list_1\n",
        "    global training_list_2\n",
        "    training_set = []\n",
        "    training_set.append(training_list_1)\n",
        "    # training_set.append(training_list_2)\n",
        "\n",
        "    global validation_list_1\n",
        "    global validation_list_2\n",
        "    validation_set = []\n",
        "    validation_set.append(validation_list_1)\n",
        "    validation_set.append(validation_list_2)\n",
        "\n",
        "    nums = len(training_list_1)\n",
        "    coefficient = np.array([[30], [1.2]])\n",
        "    x = np.array( [[1, training_list_1[i][0]] for i in range(nums)] )\n",
        "    y_ans = np.array( [[training_list_1[i][1]] for i in range(nums)] )\n",
        "    y_prediction = x.dot(coefficient)\n",
        "\n",
        "    print(y_ans.shape, y_prediction.shape, x.shape)\n",
        "    gradient = gradient_of_loss(y_ans.T, y_prediction.T, x)\n",
        "    # print((learning_rate * gradient).shape, gradient.shape, coefficient.shape)\n",
        "    # print(coefficient - learning_rate * gradient.T)\n",
        "    print(gradient[0][1] > 0.01)\n",
        "    while (gradient[0][1] > 0.0002) or (gradient[0][1] < -0.0002):\n",
        "        # print(' '.join(map(str, gradient)))\n",
        "        coefficient -= learning_rate * gradient.T\n",
        "        y_prediction = x.dot(coefficient)\n",
        "        gradient = gradient_of_loss(y_ans.T, y_prediction.T, x)\n",
        "    print(' '.join(map(str, gradient)))\n",
        "\n",
        "    coefficient -= learning_rate * gradient.T\n",
        "    y_prediction = (x.dot(coefficient)).astype(int)\n",
        "    # print(' '.join(map(str, y_prediction)))\n",
        "    print('\\n' + '-' * 30)\n",
        "    print(f'MAPE for training list 1 : {MAPE(y_ans, y_prediction)}')\n",
        "    print(f'MAPE for validation list 1 : {MAPE(np.array( [[validation_list_1[i][1]] for i in range(len(validation_list_1))]), (np.array( [[1, validation_list_1[i][0]] for i in range(len(validation_list_1))] ).dot(coefficient)) )}')\n",
        "    print(f'MAPE for validation list 2 : {MAPE(np.array( [[validation_list_2[i][1]] for i in range(len(validation_list_2))]), (np.array( [[1, validation_list_2[i][0]] for i in range(len(validation_list_2))] ).dot(coefficient)) )}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLuPxs2ZX21S"
      },
      "source": [
        "#### Step 4: Make Prediction\n",
        "\n",
        "Make prediction of testing dataset and store the values in *output_datalist*\n",
        "The final *output_datalist* should look something like this \n",
        "> [ [100], [80], ... , [90] ] where each row contains the predicted SBP\n",
        "\n",
        "Remember to also store your coefficient update in *coefficient_output*\n",
        "The final *coefficient_output* should look something like this\n",
        "> [ [1, 0, 3, 5], ... , [0.1, 0.3, 0.2, 0.5] ] where each row contains the [w0, w1, ..., wn] of your coefficient\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "8pnNDlQeYGtE"
      },
      "outputs": [],
      "source": [
        "def MakePrediction():\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IScbxxMAYAgZ"
      },
      "source": [
        "#### Step 5: Train Model and Generate Result\n",
        "\n",
        "> Notice: **Remember to output the coefficients of the model here**, otherwise 5 points would be deducted\n",
        "* If your regression model is *3x^2 + 2x^1 + 1*, your output would be:\n",
        "```\n",
        "3 2 1\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "id": "90EisOc7YG-N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "370\n",
            "(298, 1) (298, 1) (298, 2)\n",
            "True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.01681781  0.0002    ]\n",
            "\n",
            "------------------------------\n",
            "MAPE for training list 1 : 5.296185720883669\n",
            "MAPE for validation list 1 : 5.696416643952454\n",
            "MAPE for validation list 2 : 6.4556076335961405\n"
          ]
        }
      ],
      "source": [
        "PreprocessData()\n",
        "SplitData()\n",
        "GradientDescent(0.000000475)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1DpV_HcYFpl"
      },
      "source": [
        "### *Write the Output File*\n",
        "\n",
        "Write the prediction to output csv\n",
        "> Format: 'sbp'\n",
        "\n",
        "**Write the coefficient update to csv**\n",
        "> Format: 'w0', 'w1', ..., 'wn'\n",
        ">*   The number of columns is based on your number of coefficient\n",
        ">*   The number of row is based on your number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "NLSHgpDvDXNI"
      },
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "\twriter = csv.writer(csvfile)\n",
        "\tfor row in output_datalist:\n",
        "\t\twriter.writerow(row)\n",
        "\n",
        "with open(coefficient_output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "\twriter = csv.writer(csvfile)\n",
        "\tfor row in coefficient_output:\n",
        "\t\twriter.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx4408qg4xMQ"
      },
      "source": [
        "# **2. Advanced Part (40%)**\n",
        "In the second part, you need to implement the regression in a different way than the basic part to help your predictions of multiple patients SBP.\n",
        "\n",
        "You can choose **either** Matrix Inversion or Gradient Descent method.\n",
        "\n",
        "The training data will be in **hw1_advanced_training.csv** and the testing data will be in **hw1_advanced_testing.csv**.\n",
        "\n",
        "Output your prediction in **hw1_advanced.csv**\n",
        "\n",
        "Notice:\n",
        "> You cannot import any other package other than those given\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Input the training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "v66HUClZcxaE"
      },
      "outputs": [],
      "source": [
        "training_dataroot = 'hw1_advanced_training.csv' # Training data file file named as 'hw1_basic_training.csv'\n",
        "testing_dataroot = 'hw1_advanced_testing.csv'   # Testing data file named as 'hw1_basic_training.csv'\n",
        "output_dataroot = 'hw1_advanced.csv' # Output file will be named as 'hw1_basic.csv'\n",
        "\n",
        "training_datalist =  [] # Training datalist, saved as numpy array\n",
        "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
        "\n",
        "output_datalist =  [] # Your prediction, should be 220 * 1 matrix and saved as numpy array\n",
        "                      # The format of each row should be ['sbp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_data():\n",
        "    train_id = []\n",
        "    train_time = []\n",
        "    train_data = []\n",
        "\n",
        "    training_data = []; train_nums = 0\n",
        "    with open(training_dataroot, newline='') as csvfile:\n",
        "        training_data = list(csv.reader(csvfile))[1 : ]\n",
        "\n",
        "    train_id = [int(data[0]) for data in training_data]\n",
        "\n",
        "    train_data = np.array([data[2 : ] for data in training_data])\n",
        "    train_data[train_data == ''] = np.nan\n",
        "    train_data = train_data.astype(float)\n",
        "\n",
        "    for data in training_data:\n",
        "        time_list = data[1].split(' ')\n",
        "        days_ago = int(time_list[0])\n",
        "\n",
        "        time = time_list[2].split(':')\n",
        "        hour, minute = int(time[0]), int(time[1])\n",
        "        train_time.append([days_ago, hour, minute])\n",
        "\n",
        "    return train_id, train_time, train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5566, 5) 5566 5566\n"
          ]
        }
      ],
      "source": [
        "def preprocess(train_id, train_time, train_data):\n",
        "    # remove the data with more than 2 elements missing\n",
        "    i = 0; train_nums = len(train_data)\n",
        "    while i < train_nums:\n",
        "        if np.sum(np.isnan(train_data[i])) >= 2:\n",
        "            train_data = np.delete(train_data, i, 0)\n",
        "            train_id.pop(i)\n",
        "            train_time.pop(i)\n",
        "            train_nums -= 1\n",
        "            i -= 1\n",
        "        i += 1\n",
        "    print(train_data.shape, len(train_id), len(train_time))\n",
        "\n",
        "    for data in train_data:\n",
        "        if data[1] == np.nan or data[2] == np.nan or data[3] == np.nan:\n",
        "            print(data)\n",
        "            break\n",
        "        elif data[0] == np.nan:\n",
        "            print(data)\n",
        "\n",
        "    # remove outliers\n",
        "    ## We first divide the data into group_1 by id\n",
        "    # last_id = train_id[0]; group_1 = []; tem_group = []\n",
        "    # for i, data in enumerate(train_id):\n",
        "    #     tem_group.append(train_data[i])\n",
        "    #     if data != last_id:\n",
        "    #         group_1.append(tem_group)\n",
        "    #         tem_group = []\n",
        "    #         last_id = data\n",
        "    # group_1.append(tem_group)\n",
        "\n",
        "    ## Then we \n",
        "\n",
        "\n",
        "preprocess(*read_data())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Output your Prediction\n",
        "\n",
        "> your filename should be **hw1_advanced.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
        "\twriter = csv.writer(csvfile)\n",
        "\tfor row in output_datalist:\n",
        "\t\twriter.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtgCJU7FPeJL"
      },
      "source": [
        "# Report *(5%)*\n",
        "\n",
        "Report should be submitted as a pdf file **hw1_report.pdf**\n",
        "\n",
        "*   Briefly describe the difficulty you encountered\n",
        "*   Summarize your work and your reflections\n",
        "*   No more than one page\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlEE53_MPf4W"
      },
      "source": [
        "# Save the Code File\n",
        "Please save your code and submit it as an ipynb file! (**hw1.ipynb**)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
